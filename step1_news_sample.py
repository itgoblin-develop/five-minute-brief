"""
ê°œë°œ êµ¬í˜„ ëª©í‘œ
---------------
- **1ë‹¨ê³„(ë‰´ìŠ¤ ê²€ìƒ‰ë§Œ ì‚¬ìš©í•˜ëŠ” ë²„ì „)**ë¥¼ ì•„ì£¼ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•´ì„œ,
  - ìµœê·¼ Nì‹œê°„(ê¸°ë³¸ 1ì‹œê°„) ë™ì•ˆ
  - ì†Œìˆ˜ì˜ í‚¤ì›Œë“œ(ê¸°ë³¸ 5ê°œ: ê²½ì œ/ì£¼ì‹/IT/ë¶€ë™ì‚°/ì‚¬íšŒ)ë¡œ
  - ì–¼ë§ˆë‚˜ ì˜ë¯¸ ìˆëŠ” ë‰´ìŠ¤ì™€ í‚¤ì›Œë“œê°€ ë‚˜ì˜¤ëŠ”ì§€ \"ìƒ˜í”Œ\"ë¡œ ê²€ì¦í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê²ƒ.

ë¬´ì—‡ì„ í–ˆëŠ”ì§€
---------------
- ë‰´ìŠ¤ ê²€ìƒ‰ APIë§Œ ì‚¬ìš©í•´ì„œ:
  1) í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ìµœì‹  ë‰´ìŠ¤(ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 10ê°œ)ë¥¼ ê°€ì ¸ì˜¤ê³ 
  2) ì§€ì •í•œ ì‹œê°„ ë²”ìœ„(ì˜ˆ: ìµœê·¼ 1ì‹œê°„) ì•ˆì— ë“¤ì–´ì˜¤ëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§í•˜ê³ 
  3) ì œëª©/ìš”ì•½ì—ì„œ ê°„ë‹¨í•œ ê·œì¹™ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´
  4) **í‚¤ì›Œë“œ TOP 10 + ëŒ€í‘œ ë‰´ìŠ¤ ëª‡ ê°œ**ë¥¼ ì½˜ì†”ì— ì¶œë ¥.

ì–´ë–»ê²Œ í–ˆëŠ”ì§€
---------------
- `.env`ì—ì„œ `NAVER_CLIENT_ID`, `NAVER_CLIENT_SECRET`ì„ ì½ì–´ì„œ ì¸ì¦ í—¤ë” êµ¬ì„±.
- ë‰´ìŠ¤ ê²€ìƒ‰ API:
  - `GET https://openapi.naver.com/v1/search/news.json`
  - íŒŒë¼ë¯¸í„°:
    - `query`: í‚¤ì›Œë“œ (ì˜ˆ: \"ê²½ì œ\")
    - `display`: ìµœëŒ€ 10 (í…ŒìŠ¤íŠ¸ìš©, ë„ˆë¬´ ë§ì´ ì•ˆ ê°€ì ¸ì˜¤ë„ë¡)
    - `sort`: `date` (ìµœì‹ ìˆœ)
- ì‹œê°„ í•„í„°ë§:
  - `pubDate`ë¥¼ íŒŒì‹±í•´ì„œ `now - hours` ~ `now` ë²”ìœ„ì— ë“¤ì–´ê°€ëŠ” ë‰´ìŠ¤ë§Œ ì‚¬ìš©.
- í‚¤ì›Œë“œ ì¶”ì¶œ:
  - HTML íƒœê·¸ ì œê±° í›„ ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê³ 
  - í•œê¸€ 2ê¸€ì ì´ìƒ & ë¶ˆìš©ì–´ì— ì—†ëŠ” ë‹¨ì–´ë§Œ ì¹´ìš´íŠ¸
  - (ì •êµí•œ NLPê°€ ì•„ë‹ˆë¼, ì˜¤ëŠ˜ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ê°€ë³ê²Œ í™•ì¸í•˜ëŠ” ìˆ˜ì¤€)

í…ŒìŠ¤íŠ¸ ë°©ë²•
---------------
1. í”„ë¡œì íŠ¸ í´ë”ë¡œ ì´ë™
   - `cd /Users/nahyojin/Documents/GitHub/five-minute-brief`

2. (ì²˜ìŒ í•œ ë²ˆë§Œ) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
   - `pip3 install -r requirements.txt`

3. ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ìµœê·¼ 1ì‹œê°„, ê¸°ë³¸ í‚¤ì›Œë“œ 5ê°œ)
   - `python3 step1_news_sample.py`

4. ë‹¤ë¥¸ ì‹œê°„/í‚¤ì›Œë“œë¡œ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì„ ë•Œ
   - íŒŒì¼ ë§¨ ì•„ë˜ì˜ `if __name__ == "__main__":` ë¶€ë¶„ì—ì„œ:
     - `hours=3` ìœ¼ë¡œ ë°”ê¾¸ë©´ ìµœê·¼ 3ì‹œê°„
     - `test_keywords = [...]` ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•´ì„œ ì§ì ‘ í‚¤ì›Œë“œ ë„£ê¸°

5. ì¶œë ¥ì—ì„œ í™•ì¸í•  ê²ƒ
   - \"ì´ ìˆ˜ì§‘ ë‰´ìŠ¤ ê°œìˆ˜\"ê°€ ì–´ëŠ ì •ë„ ë‚˜ì˜¤ëŠ”ì§€ (ì˜ˆ: 30~100ê°œ ì •ë„)
   - \"TOP í‚¤ì›Œë“œ\" ëª©ë¡ì— ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë“¤ì´ ë‚˜ì˜¤ëŠ”ì§€
   - ëŒ€í‘œ ë‰´ìŠ¤ ì œëª©ì„ í›‘ì–´ë³´ë©´ì„œ \"ì•„ì¹¨ì— ë³´ì—¬ì£¼ë©´ ì¢‹ê² ë‹¤\" ì‹¶ì€ì§€ ê°ìœ¼ë¡œ í™•ì¸
"""

import os
import re
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from typing import Dict, List, Tuple

import requests
from dotenv import load_dotenv


def load_credentials() -> Tuple[str, str]:
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ë¥¼ ì½ì–´ì˜¨ë‹¤."""
    load_dotenv()
    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")
    if not client_id or not client_secret:
        raise RuntimeError("NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRET ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (.env í™•ì¸).")
    return client_id, client_secret


def search_news(
    client_id: str,
    client_secret: str,
    keyword: str,
    display: int = 10,
) -> List[Dict]:
    """ë‰´ìŠ¤ ê²€ìƒ‰ APIë¡œ íŠ¹ì • í‚¤ì›Œë“œì˜ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¨ë‹¤."""
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": keyword,
        "display": min(display, 10),
        "sort": "date",
    }
    resp = requests.get(url, headers=headers, params=params, timeout=10)
    if not resp.ok:
        raise RuntimeError(f"ë‰´ìŠ¤ ê²€ìƒ‰ API ì˜¤ë¥˜ (status={resp.status_code}): {resp.text[:200]}")
    data = resp.json()
    return data.get("items", [])


def filter_news_by_time(
    news_items: List[Dict],
    start_time: datetime,
    end_time: datetime,
) -> List[Dict]:
    """ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ pubDate ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ ë²”ìœ„ ì•ˆì— ìˆëŠ” ê²ƒë§Œ ë‚¨ê¸´ë‹¤."""
    filtered: List[Dict] = []
    for item in news_items:
        try:
            pub_date_str = item.get("pubDate", "")
            # ì˜ˆ: "Mon, 26 Jan 2026 05:30:00 +0900"
            base = pub_date_str.split(" +")[0]
            pub_date = datetime.strptime(base, "%a, %d %b %Y %H:%M:%S")
        except Exception:
            continue
        if start_time <= pub_date <= end_time:
            filtered.append(item)
    return filtered


def extract_keywords_from_text(text: str) -> List[str]:
    """ì•„ì£¼ ê°„ë‹¨í•œ ê·œì¹™ìœ¼ë¡œ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•œë‹¤."""
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r"<[^>]+>", "", text)

    # ë¶ˆìš©ì–´ (í…ŒìŠ¤íŠ¸ìš©, ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤)
    stopwords = {
        "ì˜¤ëŠ˜",
        "ë‚´ì¼",
        "ì–´ì œ",
        "ì˜",
        "ì´",
        "ê°€",
        "ì„",
        "ë¥¼",
        "ì€",
        "ëŠ”",
        "ì—",
        "ì—ì„œ",
        "ë¡œ",
        "ìœ¼ë¡œ",
        "ì™€",
        "ê³¼",
        "ë„",
        "ë§Œ",
        "ê¹Œì§€",
        "ë¶€í„°",
        "ë³´ë‹¤",
        "ê°™ì´",
        "ê·¸ë¦¬ê³ ",
        "í•˜ì§€ë§Œ",
        "ìµœê·¼",
        "í˜„ì¬",
        "ìˆë‹¤",
        "ì—†ë‹¤",
        "ëŒ€í•œ",
        "ê´€ë ¨",
        "í†µí•´",
        "êµ­ë‚´",
        "ê°œìµœ",
    }

    words = text.split()
    keywords: List[str] = []

    for w in words:
        # í•œê¸€ 2ê¸€ì ì´ìƒ + ë¶ˆìš©ì–´ ì œì™¸
        if re.match(r"^[ê°€-í£]{2,}$", w) and w not in stopwords:
            keywords.append(w)
        # ì˜ë¬¸ ëŒ€ë¬¸ì 3ê¸€ì ì´ìƒ (ì˜ˆ: ETF, GDP ë“±)
        elif re.match(r"^[A-Z]{3,}$", w):
            keywords.append(w)

    return keywords


def analyze_keywords(news_list: List[Dict]) -> List[Dict]:
    """ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ í›„ ìƒìœ„ í‚¤ì›Œë“œì™€ ëŒ€í‘œ ë‰´ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤."""
    freq: Counter = Counter()
    mapping: defaultdict = defaultdict(list)

    for item in news_list:
        title = item.get("title", "")
        desc = item.get("description", "")

        title_kws = extract_keywords_from_text(title)
        desc_kws = extract_keywords_from_text(desc)

        # ì œëª©ì€ ê°€ì¤‘ì¹˜ 3
        for k in title_kws:
            freq[k] += 3
            if item not in mapping[k]:
                mapping[k].append(item)

        # ìš”ì•½ì€ ê°€ì¤‘ì¹˜ 1
        for k in desc_kws:
            freq[k] += 1
            if item not in mapping[k]:
                mapping[k].append(item)

    # ìƒìœ„ 20ê°œ ì •ë„ë§Œ ë³´ê¸°
    top_raw = freq.most_common(20)

    results: List[Dict] = []
    for keyword, count in top_raw:
        related = mapping[keyword]
        # ë„ˆë¬´ ì•½í•œ í‚¤ì›Œë“œëŠ” ê±°ë¥´ê¸° (ë¹ˆë„ 5 ë¯¸ë§Œ & ê´€ë ¨ ë‰´ìŠ¤ 1ê°œ ì´ëŸ° ê²½ìš°)
        if count < 5 or len(related) < 2:
            continue

        # ìµœì‹  ë‰´ìŠ¤ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ìƒìœ„ 3ê°œë§Œ
        sorted_news = sorted(
            related,
            key=lambda x: x.get("pubDate", ""),
            reverse=True,
        )[:3]

        results.append(
            {
                "keyword": keyword,
                "frequency": count,
                "news_count": len(related),
                "top_news": sorted_news,
            }
        )

    return results


def run_sample(hours: int = 1, test_keywords: List[str] | None = None) -> None:
    """1ë‹¨ê³„ ìƒ˜í”Œ ì‹¤í–‰ í•¨ìˆ˜."""
    if test_keywords is None:
        test_keywords = ["ê²½ì œ", "ì£¼ì‹", "IT", "ë¶€ë™ì‚°", "ì‚¬íšŒ"]

    client_id, client_secret = load_credentials()

    now = datetime.now()
    start_time = now - timedelta(hours=hours)

    print("=" * 60)
    print("ğŸ§ª 1ë‹¨ê³„ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (ë‰´ìŠ¤ ê²€ìƒ‰ APIë§Œ ì‚¬ìš©)")
    print("=" * 60)
    print(f"ì‹œê°„ ë²”ìœ„: {start_time.strftime('%Y-%m-%d %H:%M')} ~ {now.strftime('%Y-%m-%d %H:%M')}")
    print(f"í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ: {', '.join(test_keywords)}")
    print(f"ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 10ê°œ ë‰´ìŠ¤, ì´ API í˜¸ì¶œ ì˜ˆìƒ: {len(test_keywords)}íšŒ")
    print("-" * 60)

    all_news: List[Dict] = []
    seen_links: set[str] = set()

    for kw in test_keywords:
        print(f"ê²€ìƒ‰ ì¤‘: '{kw}'...", end=" ")
        try:
            items = search_news(client_id, client_secret, kw, display=10)
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            continue

        filtered = filter_news_by_time(items, start_time, now)
        for n in filtered:
            link = n.get("link", "")
            if link and link not in seen_links:
                seen_links.add(link)
                all_news.append(n)

        print(f"âœ… {len(filtered)}ê°œ (ì „ì²´: {len(all_news)}ê°œ)")

    print("\n" + "=" * 60)
    print(f"âœ… ì´ {len(all_news)}ê°œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    print("=" * 60)

    if not all_news:
        print("âš ï¸  ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œê°„ ë²”ìœ„ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ í‚¤ì›Œë“œë¥¼ ë°”ê¿”ë³´ì„¸ìš”.")
        return

    # í‚¤ì›Œë“œ ë¶„ì„
    print("\nğŸ” í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
    trending = analyze_keywords(all_news)

    print("\n" + "=" * 60)
    print("ğŸ”¥ 1ë‹¨ê³„ ìƒ˜í”Œ - ì£¼ìš” í‚¤ì›Œë“œ TOP 10 (ë‰´ìŠ¤ ê²€ìƒ‰ ê¸°ë°˜)")
    print("=" * 60)

    if not trending:
        print("âš ï¸  ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ í‚¤ì›Œë“œë¥¼ ë” ì¢íˆê±°ë‚˜, ì‹œê°„ ë²”ìœ„ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.")
        return

    for i, item in enumerate(trending[:10], 1):
        print(f"\n{i}. í‚¤ì›Œë“œ: {item['keyword']}")
        print(f"   ë¹ˆë„ìˆ˜: {item['frequency']}")
        print(f"   ê´€ë ¨ ë‰´ìŠ¤: {item['news_count']}ê°œ")
        print("   ëŒ€í‘œ ë‰´ìŠ¤:")
        for j, n in enumerate(item["top_news"], 1):
            title = n.get("title", "").replace("<b>", "").replace("</b>", "")
            pub_date = n.get("pubDate", "")
            print(f"     {j}. {title}")
            print(f"        ë°œí–‰: {pub_date}")


if __name__ == "__main__":
    # ê¸°ë³¸: ìµœê·¼ 1ì‹œê°„, ê¸°ë³¸ í‚¤ì›Œë“œ 5ê°œ
    run_sample(hours=1)

