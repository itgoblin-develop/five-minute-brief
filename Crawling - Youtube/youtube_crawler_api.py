"""
ìœ íŠœë¸Œ í¬ë¡¤ëŸ¬ v2 (YouTube Data API ì‚¬ìš©)
- ê³µì‹ APIë¡œ ì •í™•í•œ ì˜ìƒ ê²€ìƒ‰ ë° í•„í„°ë§
- ìë§‰ ì¶”ì¶œì€ youtube-transcript-api ì‚¬ìš©
- í•œêµ­ì–´ ìë§‰(CC) ìœ ë¬´ í™•ì¸ ê°€ëŠ¥
"""

import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional
import yaml

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env ì‚¬ìš©)
try:
    from dotenv import load_dotenv
    env_path = Path(__file__).resolve().parent.parent / ".env"
    load_dotenv(dotenv_path=env_path)
except ImportError:
    print("âš ï¸ python-dotenv ì„¤ì¹˜ í•„ìš”: pip install python-dotenv")

# YouTube Data API
try:
    from googleapiclient.discovery import build
    YOUTUBE_API_AVAILABLE = True
except ImportError:
    YOUTUBE_API_AVAILABLE = False
    print("âš ï¸ google-api-python-client ì„¤ì¹˜ í•„ìš”")

# YouTube Transcript API
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    from youtube_transcript_api._errors import (
        TranscriptsDisabled,
        NoTranscriptFound,
        VideoUnavailable
    )
    TRANSCRIPT_AVAILABLE = True
except ImportError:
    TRANSCRIPT_AVAILABLE = False
    print("âš ï¸ youtube-transcript-api ì„¤ì¹˜ í•„ìš”")


class YouTubeCrawler:
    """YouTube Data API ê¸°ë°˜ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv('YOUTUBE_API_KEY')
        if not self.api_key or self.api_key == 'ì—¬ê¸°ì—_API_í‚¤_ì…ë ¥':
            raise ValueError("YouTube API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. .env íŒŒì¼ì— YOUTUBE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        
        self.youtube = build('youtube', 'v3', developerKey=self.api_key)
        self.transcript_api = YouTubeTranscriptApi() if TRANSCRIPT_AVAILABLE else None
    
    def search_videos(
        self,
        keyword: str,
        max_results: int = 10,
        published_after_hours: int = 48,
        video_duration: str = 'medium',  # short(<4min), medium(4-20min), long(>20min)
        caption: str = 'closedCaption',  # any, closedCaption, none
        region_code: str = 'KR',
        relevance_language: str = 'ko'
    ) -> list[dict]:
        """
        í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            published_after_hours: ìµœê·¼ Nì‹œê°„ ì´ë‚´ ì˜ìƒë§Œ
            video_duration: ì˜ìƒ ê¸¸ì´ (short/medium/long)
            caption: ìë§‰ ìœ ë¬´ (closedCaption = ìë§‰ ìˆëŠ” ì˜ìƒë§Œ)
            region_code: ì§€ì—­ ì½”ë“œ (KR = í•œêµ­)
            relevance_language: ê´€ë ¨ ì–¸ì–´
        """
        # ì‹œê°„ í•„í„°
        published_after = (datetime.utcnow() - timedelta(hours=published_after_hours)).isoformat() + 'Z'
        
        try:
            search_response = self.youtube.search().list(
                q=keyword,
                part='snippet',
                type='video',
                maxResults=max_results,
                order='relevance',
                publishedAfter=published_after,
                videoDuration=video_duration,
                videoCaption=caption,
                regionCode=region_code,
                relevanceLanguage=relevance_language
            ).execute()
            
            video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
            
            if not video_ids:
                return []
            
            # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            videos_response = self.youtube.videos().list(
                part='snippet,contentDetails,statistics',
                id=','.join(video_ids)
            ).execute()
            
            videos = []
            for item in videos_response.get('items', []):
                video = {
                    'video_id': item['id'],
                    'title': item['snippet']['title'],
                    'description': item['snippet']['description'],  # ì „ì²´ ì„¤ëª… ìˆ˜ì§‘
                    'channel_name': item['snippet']['channelTitle'],
                    'channel_id': item['snippet']['channelId'],
                    'published_at': item['snippet']['publishedAt'],
                    'duration': item['contentDetails']['duration'],
                    'view_count': int(item['statistics'].get('viewCount', 0)),
                    'like_count': int(item['statistics'].get('likeCount', 0)),
                    'comment_count': int(item['statistics'].get('commentCount', 0)),
                    'link': f"https://www.youtube.com/watch?v={item['id']}",
                    'search_keyword': keyword
                }
                videos.append(video)
            
            return videos
            
        except Exception as e:
            print(f"âŒ API ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_transcript(self, video_id: str, korean_only: bool = True) -> Optional[dict]:
        """
        ì˜ìƒ ìë§‰ ì¶”ì¶œ
        
        Args:
            video_id: ìœ íŠœë¸Œ ì˜ìƒ ID
            korean_only: Trueë©´ í•œêµ­ì–´ ìë§‰ë§Œ ì¶”ì¶œ
        """
        if not self.transcript_api:
            return None
        
        try:
            # ìë§‰ ëª©ë¡ ì¡°íšŒ
            transcript_list_obj = self.transcript_api.list(video_id)
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ìë§‰ ì–¸ì–´ í™•ì¸
            available_transcripts = list(transcript_list_obj)
            
            # í•œêµ­ì–´ ìë§‰ ì°¾ê¸°
            korean_transcript = None
            for t in available_transcripts:
                lang_code = t.language_code.lower()
                if lang_code in ['ko', 'ko-kr']:
                    korean_transcript = t
                    break
            
            if korean_only and not korean_transcript:
                print(f"      âš ï¸ í•œêµ­ì–´ ìë§‰ ì—†ìŒ")
                return None
            
            # ìë§‰ ì„ íƒ (í•œêµ­ì–´ ìš°ì„ , ì—†ìœ¼ë©´ ì²« ë²ˆì§¸)
            selected = korean_transcript if korean_transcript else available_transcripts[0]
            transcript_data = selected.fetch()
            
            # ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            transcript_items = []
            for snippet in transcript_data:
                transcript_items.append({
                    'text': snippet.text,
                    'start': snippet.start,
                    'duration': snippet.duration
                })
            
            full_text = ' '.join([t['text'] for t in transcript_items])
            
            return {
                'language': selected.language_code,
                'transcript': transcript_items,
                'full_text': full_text,
                'word_count': len(full_text.split())
            }
            
        except TranscriptsDisabled:
            print(f"      âš ï¸ ìë§‰ ë¹„í™œì„±í™”")
        except NoTranscriptFound:
            print(f"      âš ï¸ ìë§‰ ì—†ìŒ")
        except VideoUnavailable:
            print(f"      âš ï¸ ì˜ìƒ ì ‘ê·¼ ë¶ˆê°€")
        except Exception as e:
            print(f"      âš ï¸ ìë§‰ ì˜¤ë¥˜: {e}")
        
        return None
    
    def calculate_quality_score(self, video: dict) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ì¡°íšŒìˆ˜ (30%)
        views = video.get('view_count', 0)
        if views >= 100000:
            score += 30
        elif views >= 50000:
            score += 24
        elif views >= 10000:
            score += 18
        else:
            score += 12
        
        # ì¢‹ì•„ìš” ë¹„ìœ¨ (30%)
        likes = video.get('like_count', 0)
        if views > 0:
            like_ratio = likes / views
            score += min(30, like_ratio * 1000)
        
        # ìµœì‹ ì„± (20%)
        try:
            published = datetime.fromisoformat(video['published_at'].replace('Z', '+00:00'))
            hours_ago = (datetime.now(published.tzinfo) - published).total_seconds() / 3600
            if hours_ago <= 12:
                score += 20
            elif hours_ago <= 24:
                score += 16
            elif hours_ago <= 48:
                score += 12
        except:
            score += 10
        
        # ì±„ë„ (20%) - ê¸°ë³¸ ì ìˆ˜
        score += 15
        
        return round(score, 1)


def load_config(config_path: str) -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    with open(config_path, 'r', encoding='utf-8') as f:
        content = f.read()
        lines = content.split('\n')
        yaml_lines = []
        in_yaml = False
        for line in lines:
            if line.strip().startswith('categories:'):
                in_yaml = True
            if in_yaml:
                yaml_lines.append(line)
        return yaml.safe_load('\n'.join(yaml_lines))


def crawl_with_api(
    config_path: str,
    output_path: str,
    videos_per_keyword: int = 5,
    categories: list[str] = None
) -> dict:
    """API ê¸°ë°˜ í¬ë¡¤ë§ ì‹¤í–‰"""
    
    print("=" * 60)
    print("ğŸš€ YouTube API í¬ë¡¤ëŸ¬ v2 ì‹œì‘")
    print("=" * 60)
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    try:
        crawler = YouTubeCrawler()
    except ValueError as e:
        print(f"âŒ {e}")
        return {}
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(config_path)
    if not config:
        print("âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
        return {}
    
    result = {
        'crawled_at': datetime.now().isoformat(),
        'api_version': 'YouTube Data API v3',
        'total_videos': 0,
        'total_with_transcript': 0,
        'categories': []
    }
    
    collected_ids = set()
    categories_config = config.get('categories', {})
    target_categories = categories or list(categories_config.keys())
    
    for cat_key in target_categories:
        if cat_key not in categories_config:
            continue
        
        cat_config = categories_config[cat_key]
        cat_name = cat_config.get('name', cat_key)
        
        print(f"\nğŸ“ ì¹´í…Œê³ ë¦¬: {cat_name}")
        print("-" * 40)
        
        category_data = {
            'category_key': cat_key,
            'category_name': cat_name,
            'videos': []
        }
        
        # í‚¤ì›Œë“œ ìˆ˜ì§‘
        keywords_config = cat_config.get('keywords', {})
        exclude_keywords = cat_config.get('exclude_keywords', [])
        
        all_keywords = []
        for priority in ['priority_1', 'priority_2']:
            all_keywords.extend(keywords_config.get(priority, []))
        
        for keyword in all_keywords[:3]:
            print(f"\nğŸ” í‚¤ì›Œë“œ: '{keyword}' (ìë§‰ ìˆëŠ” ì˜ìƒë§Œ)")
            
            # API ê²€ìƒ‰ (ìë§‰ ìˆëŠ” ì˜ìƒë§Œ)
            videos = crawler.search_videos(
                keyword=keyword,
                max_results=videos_per_keyword,
                published_after_hours=72,  # ìµœê·¼ 3ì¼
                video_duration='medium',   # 4~20ë¶„
                caption='closedCaption'    # ìë§‰ ìˆëŠ” ì˜ìƒë§Œ!
            )
            
            print(f"   â†’ {len(videos)}ê°œ ì˜ìƒ ë°œê²¬")
            
            for video in videos:
                # ì¤‘ë³µ ì²´í¬
                if video['video_id'] in collected_ids:
                    continue
                
                # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
                title = video.get('title', '')
                is_excluded = any(ex.lower() in title.lower() for ex in exclude_keywords)
                if is_excluded:
                    print(f"      â›” ì œì™¸: {title[:40]}...")
                    continue
                
                collected_ids.add(video['video_id'])
                video['category'] = cat_key
                video['quality_score'] = crawler.calculate_quality_score(video)
                
                # ìë§‰ ì¶”ì¶œ
                print(f"   ğŸ“ ìë§‰: {title[:40]}...")
                transcript_data = crawler.get_transcript(video['video_id'])
                
                if transcript_data:
                    video['has_captions'] = True
                    video['transcript'] = transcript_data
                    result['total_with_transcript'] += 1
                    print(f"      âœ… {transcript_data['word_count']}ë‹¨ì–´ ì¶”ì¶œ")
                else:
                    video['has_captions'] = False
                    video['transcript'] = None
                
                video['fetched_at'] = datetime.now().isoformat()
                category_data['videos'].append(video)
                result['total_videos'] += 1
        
        result['categories'].append(category_data)
    
    # JSON ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 60)
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   - ì´ ì˜ìƒ: {result['total_videos']}ê°œ")
    print(f"   - ìë§‰ ì¶”ì¶œ ì„±ê³µ: {result['total_with_transcript']}ê°œ")
    print(f"   - ì €ì¥: {output_path}")
    print("=" * 60)
    
    return result


if __name__ == '__main__':
    base_dir = Path(__file__).parent
    
    # .env íŒŒì¼ ë¡œë“œ
    env_path = base_dir / '.env'
    if env_path.exists():
        from dotenv import load_dotenv
        load_dotenv(env_path)
    
    crawl_with_api(
        config_path=str(base_dir / 'config.yaml'),
        output_path=str(base_dir / 'youtube_data.json'),
        videos_per_keyword=5,
        categories=['economy', 'trend']
    )
