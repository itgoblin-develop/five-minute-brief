#!/usr/bin/env python3
"""
Phase 3: AI ì¬êµ¬ì„± ëª¨ë“ˆ
- LLM í´ë¼ì´ì–¸íŠ¸ (Gemini, OpenAI) ì¶”ìƒí™”
- LLMRouter: ë©”ì¸/í´ë°± ìë™ ì „í™˜ + ì¬ì‹œë„
- AIRewriter: í´ëŸ¬ìŠ¤í„° â†’ ì¬êµ¬ì„± ê¸°ì‚¬ ë³€í™˜
"""

import json
import os
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, List, Optional


class LLMClient(ABC):
    """LLM API í´ë¼ì´ì–¸íŠ¸ ì¶”ìƒ í´ë˜ìŠ¤"""

    @abstractmethod
    def generate(self, system_prompt: str, user_prompt: str) -> dict:
        """JSON í˜•íƒœì˜ ì‘ë‹µì„ ë°˜í™˜"""
        pass

    @abstractmethod
    def name(self) -> str:
        """í´ë¼ì´ì–¸íŠ¸ ì´ë¦„ ë°˜í™˜"""
        pass


class GeminiClient(LLMClient):
    """Google Gemini API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, api_key: str = None, temperature: float = 0.7, max_output_tokens: int = 2048):
        import google.generativeai as genai

        api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash")
        self.temperature = temperature
        self.max_output_tokens = max_output_tokens

    def generate(self, system_prompt: str, user_prompt: str) -> dict:
        response = self.model.generate_content(
            f"{system_prompt}\n\n{user_prompt}",
            generation_config={
                "response_mime_type": "application/json",
                "temperature": self.temperature,
                "max_output_tokens": self.max_output_tokens,
            }
        )
        return json.loads(response.text)

    def name(self) -> str:
        return "gemini"


class OpenAIClient(LLMClient):
    """OpenAI API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, api_key: str = None, temperature: float = 0.7, max_tokens: int = 2048):
        from openai import OpenAI

        api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.client = OpenAI(api_key=api_key)
        self.temperature = temperature
        self.max_tokens = max_tokens

    def generate(self, system_prompt: str, user_prompt: str) -> dict:
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            response_format={"type": "json_object"},
            temperature=self.temperature,
            max_tokens=self.max_tokens,
        )
        return json.loads(response.choices[0].message.content)

    def name(self) -> str:
        return "openai"


class LLMRouter:
    """ë©”ì¸/í´ë°± LLM ìë™ ì „í™˜ ë¼ìš°í„° (ì¬ì‹œë„ ë¡œì§ í†µí•©)"""

    def __init__(self, primary: LLMClient = None, fallback: LLMClient = None,
                 max_retries: int = 3, retry_delay_base: int = 1):
        self.primary = primary
        self.fallback = fallback
        self.max_retries = max_retries
        self.retry_delay_base = retry_delay_base

        # í†µê³„
        self.stats = {
            "primary_success": 0,
            "primary_fail": 0,
            "fallback_success": 0,
            "fallback_fail": 0,
        }

    def generate(self, system_prompt: str, user_prompt: str) -> dict:
        """ì¬ì‹œë„ + í´ë°±ì´ í†µí•©ëœ LLM í˜¸ì¶œ"""

        # ë©”ì¸ API ì¬ì‹œë„
        if self.primary:
            for attempt in range(self.max_retries):
                try:
                    result = self.primary.generate(system_prompt, user_prompt)
                    self.stats["primary_success"] += 1
                    return result
                except Exception as e:
                    wait = min(self.retry_delay_base * (2 ** attempt), 30)
                    print(f"  âš ï¸ {self.primary.name()} ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {e}")
                    self.stats["primary_fail"] += 1
                    if attempt < self.max_retries - 1:
                        time.sleep(wait)

        # í´ë°± API ì‹œë„
        if self.fallback:
            print(f"  ğŸ”„ í´ë°± API ({self.fallback.name()})ë¡œ ì „í™˜")
            for attempt in range(self.max_retries):
                try:
                    result = self.fallback.generate(system_prompt, user_prompt)
                    self.stats["fallback_success"] += 1
                    return result
                except Exception as e:
                    wait = min(self.retry_delay_base * (2 ** attempt), 30)
                    print(f"  âš ï¸ {self.fallback.name()} í´ë°± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {e}")
                    self.stats["fallback_fail"] += 1
                    if attempt < self.max_retries - 1:
                        time.sleep(wait)

        raise RuntimeError("ëª¨ë“  LLM API í˜¸ì¶œ ì‹¤íŒ¨")

    def get_stats(self) -> dict:
        return self.stats.copy()


def build_articles_block(cluster: List[dict], max_chars_per_article: int = 1500) -> str:
    """í´ëŸ¬ìŠ¤í„° ë‚´ ê¸°ì‚¬ë“¤ì„ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•  í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ ë³€í™˜"""
    blocks = []
    for i, article in enumerate(cluster, 1):
        content = article.get("content", "")[:max_chars_per_article]
        block = f"""--- ê¸°ì‚¬ {i} ---
[ì œëª©] {article.get('title', '')}
[ì¶œì²˜] {article.get('press', 'ì•Œ ìˆ˜ ì—†ìŒ')}
[ë‚´ìš©]
{content}
"""
        blocks.append(block)
    return "\n".join(blocks)


class AIRewriter:
    """AI ë‰´ìŠ¤ ì¬êµ¬ì„±ê¸° (í˜ë¥´ì†Œë‚˜ ê¸°ë°˜)"""

    # ì¹´í…Œê³ ë¦¬ë³„ ë‹´ë‹¹ í˜ë¥´ì†Œë‚˜ ê¸°ë³¸ ë§¤í•‘
    DEFAULT_PERSONA_MAP = {
        "Tech": "ë¯¼ì¤€",
        "AI": "í˜„ìš°",
        "Dev": "ìˆ˜ì§„",
        "Product": "ë„ìœ¤",
        "Security": "ì§€ì€",
    }

    def __init__(self, llm_router: LLMRouter, config: dict = None):
        self.llm = llm_router
        self.config = config or {}
        self.request_interval = self.config.get("request_interval", 0.5)
        self.persona_map = self.config.get("persona_map", self.DEFAULT_PERSONA_MAP)

        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        self.system_prompt = self._load_prompt("system_prompt.txt")
        self.merge_template = self._load_prompt("merge_prompt.txt")
        self.single_template = self._load_prompt("single_prompt.txt")

    def _load_prompt(self, filename: str) -> str:
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
        prompt_dir = Path(__file__).parent / "prompts"
        prompt_path = prompt_dir / filename
        if not prompt_path.exists():
            raise FileNotFoundError(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_path}")
        with open(prompt_path, "r", encoding="utf-8") as f:
            return f.read()

    def reconstruct_cluster(self, cluster: List[dict], category: str) -> Optional[dict]:
        """í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ì¬êµ¬ì„± ê¸°ì‚¬ë¡œ ë³€í™˜"""

        if not cluster:
            return None

        if len(cluster) == 1:
            article = cluster[0]
            user_prompt = self.single_template.format(
                category=category,
                title=article.get("title", ""),
                press=article.get("press", "ì•Œ ìˆ˜ ì—†ìŒ"),
                content=article.get("content", "")[:2000],
            )
        else:
            articles_block = build_articles_block(cluster)
            user_prompt = self.merge_template.format(
                article_count=len(cluster),
                category=category,
                articles_block=articles_block,
            )

        # LLM í˜¸ì¶œ (LLMRouterê°€ ì¬ì‹œë„+í´ë°± ì²˜ë¦¬)
        try:
            result = self.llm.generate(self.system_prompt, user_prompt)

            # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ í™•ì¸
            required_fields = ["title", "summary", "bullet_summary", "content", "hashtags"]
            for field in required_fields:
                if field not in result:
                    print(f"  âš ï¸ LLM ì‘ë‹µì— '{field}' í•„ë“œ ëˆ„ë½")
                    result[field] = self._get_default_value(field, cluster)

            # í˜ë¥´ì†Œë‚˜ ì •ë³´ ë³´ì¥
            if "persona" not in result:
                result["persona"] = self.persona_map.get(category, "ë¯¼ì¤€")

            return result
        except RuntimeError:
            print(f"  âŒ í´ëŸ¬ìŠ¤í„° ì¬êµ¬ì„± ì‹¤íŒ¨ (ê¸°ì‚¬ {len(cluster)}ê±´), í´ë°± ì¬êµ¬ì„± ì‚¬ìš©")
            return self._fallback_reconstruct(cluster, category)

    def _get_default_value(self, field: str, cluster: List[dict]):
        """ëˆ„ë½ëœ í•„ë“œì˜ ê¸°ë³¸ê°’ ìƒì„±"""
        representative = max(cluster, key=lambda a: a.get("trend_score", 0))

        if field == "title":
            return representative.get("title", "ì œëª© ì—†ìŒ")[:30]
        elif field == "summary":
            return representative.get("content", "")[:200]
        elif field == "bullet_summary":
            return ["ìš”ì•½ ì •ë³´ 1", "ìš”ì•½ ì •ë³´ 2", "ìš”ì•½ ì •ë³´ 3"]
        elif field == "content":
            return representative.get("content", "")[:600]
        elif field == "hashtags":
            return representative.get("matched_keywords", [])[:5] or ["ë‰´ìŠ¤"]
        return ""

    def _fallback_reconstruct(self, cluster: List[dict], category: str) -> dict:
        """LLM ì™„ì „ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê¸°ë°˜ ê°„ì´ ì¬êµ¬ì„±"""
        representative = max(cluster, key=lambda a: a.get("trend_score", 0))
        content = representative.get("content", "")

        # ë¬¸ì¥ ê²½ê³„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        sentences = [s.strip() for s in content.replace('.\n', '. ').split('.') if s.strip()]

        return {
            "title": representative.get("title", "")[:30],
            "summary": '. '.join(sentences[:2]) + '.' if len(sentences) >= 2 else content[:200],
            "bullet_summary": [
                sentences[0][:40] if len(sentences) > 0 else "ìš”ì•½ ìƒì„± ë¶ˆê°€",
                sentences[1][:40] if len(sentences) > 1 else "ì¶”ê°€ ì •ë³´ ì—†ìŒ",
                sentences[2][:40] if len(sentences) > 2 else "ìƒì„¸ ë‚´ìš©ì€ ì›ë¬¸ ì°¸ì¡°",
            ],
            "content": '. '.join(sentences[:6]) + '.' if len(sentences) >= 6 else content[:600],
            "hashtags": representative.get("matched_keywords", [])[:5] or ["IT"],
            "persona": self.persona_map.get(category, "ë¯¼ì¤€"),
            "_fallback": True,
        }

    def reconstruct_all(self, clustered_data: Dict[str, List[List[dict]]]) -> List[dict]:
        """ì „ì²´ ì¹´í…Œê³ ë¦¬ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ìˆœì°¨ ì¬êµ¬ì„±"""
        results = []
        total_clusters = sum(len(clusters) for clusters in clustered_data.values())
        processed = 0

        for category, clusters in clustered_data.items():
            for cluster in clusters:
                processed += 1
                print(f"  ğŸ”„ [{processed}/{total_clusters}] {category} - {len(cluster)}ê±´ ë³‘í•© ì¤‘...")

                result = self.reconstruct_cluster(cluster, category)
                if result:
                    result["category"] = category
                    result["source_count"] = len(cluster)
                    result["source_links"] = [a.get("link", "") for a in cluster]
                    result["_source_articles"] = cluster
                    results.append(result)

                # Rate limit ëŒ€ì‘
                if processed < total_clusters:
                    time.sleep(self.request_interval)

        fallback_count = sum(1 for r in results if r.get("_fallback"))
        print(f"  ğŸ“Š ì¬êµ¬ì„± ê²°ê³¼: {len(results)}ê±´ (í´ë°±: {fallback_count}ê±´)")

        return results


def create_llm_router(config: dict = None) -> LLMRouter:
    """ì„¤ì • ê¸°ë°˜ LLMRouter ìƒì„± íŒ©í† ë¦¬"""
    config = config or {}
    primary_type = config.get("primary", os.getenv("LLM_PRIMARY", "gemini"))
    fallback_type = config.get("fallback", os.getenv("LLM_FALLBACK", "openai"))
    temperature = config.get("temperature", 0.7)
    max_output_tokens = config.get("max_output_tokens", 2048)
    max_retries = config.get("retry_count", 3)
    retry_delay_base = config.get("retry_delay_base", 1)

    clients = {}

    # í•„ìš”í•œ í´ë¼ì´ì–¸íŠ¸ë§Œ ìƒì„±
    needed = set([primary_type, fallback_type]) - {"none", ""}
    for client_type in needed:
        try:
            if client_type == "gemini":
                clients["gemini"] = GeminiClient(
                    temperature=temperature,
                    max_output_tokens=max_output_tokens,
                )
            elif client_type == "openai":
                clients["openai"] = OpenAIClient(
                    temperature=temperature,
                    max_tokens=max_output_tokens,
                )
        except (ValueError, ImportError) as e:
            print(f"  âš ï¸ {client_type} í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    primary = clients.get(primary_type)
    fallback = clients.get(fallback_type)

    if not primary and not fallback:
        print("  âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ LLM APIê°€ ì—†ìŠµë‹ˆë‹¤. í´ë°± ì¬êµ¬ì„±ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")

    return LLMRouter(
        primary=primary,
        fallback=fallback,
        max_retries=max_retries,
        retry_delay_base=retry_delay_base,
    )
