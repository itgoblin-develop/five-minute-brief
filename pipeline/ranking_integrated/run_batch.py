import argparse
import json
import os
import sys
import subprocess
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€í•˜ì—¬ generate_briefing ëª¨ë“ˆ import
current_dir = Path(__file__).resolve().parent
sys.path.append(str(current_dir))

try:
    from generate_briefing import TrendCollector, BriefingGenerator
except ImportError as e:
    print(f"âŒ generate_briefing ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    sys.exit(1)

def parse_args():
    parser = argparse.ArgumentParser(description='Daily Trend Briefing Batch Job')
    parser.add_argument('--start', type=str, required=True, help='Start DateTime (YYYY-MM-DD HH:MM)')
    parser.add_argument('--end', type=str, required=True, help='End DateTime (YYYY-MM-DD HH:MM)')
    parser.add_argument('--skip-crawl', action='store_true', help='Skip crawling and use existing data')
    return parser.parse_args()

def parse_korean_datetime(date_str: str) -> datetime:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ë“±ì˜ ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±
    ì˜ˆ: '2024.02.02. ì˜¤ì „ 10:30', '1ì‹œê°„ ì „', '5ë¶„ ì „'
    """
    now = datetime.now()
    date_str = date_str.strip()
    
    # 1. "Në¶„ ì „", "Nì‹œê°„ ì „", "Nì¼ ì „" ì²˜ë¦¬ (ê³µë°± ìœ ë¬´ ìƒê´€ì—†ì´)
    if 'ë¶„' in date_str and 'ì „' in date_str:
        try:
            minutes = int(re.search(r'(\d+)', date_str).group(1))
            return now - timedelta(minutes=minutes)
        except:
            pass
    elif 'ì‹œê°„' in date_str and 'ì „' in date_str:
        try:
            hours = int(re.search(r'(\d+)', date_str).group(1))
            return now - timedelta(hours=hours)
        except:
            pass
    elif 'ì¼' in date_str and 'ì „' in date_str:
        try:
            days = int(re.search(r'(\d+)', date_str).group(1))
            return now - timedelta(days=days)
        except:
            pass
    
    # 2. "2024.02.02. ì˜¤ì „ 10:30" í˜•ì‹ ì²˜ë¦¬
    try:
        # 'ì˜¤ì „', 'ì˜¤í›„' ì²˜ë¦¬
        is_pm = 'ì˜¤í›„' in date_str
        cleaned = re.sub(r'(ì˜¤ì „|ì˜¤í›„)\s*', '', date_str).strip()
        # ì (.)ì´ ë§ˆì§€ë§‰ì— ìˆì„ ìˆ˜ ìˆìŒ
        cleaned = cleaned.rstrip('.')
        
        # í¬ë§· ë§ì¶”ê¸° (YYYY.MM.DD HH:MM)
        dt = datetime.strptime(cleaned, "%Y.%m.%d. %H:%M")
        
        if is_pm and dt.hour < 12:
            dt = dt + timedelta(hours=12)
        elif not is_pm and dt.hour == 12: # ì˜¤ì „ 12ì‹œëŠ” 0ì‹œ
            dt = dt - timedelta(hours=12)
            
        return dt
    except:
        pass

    # 3. YYYY-MM-DD í˜•ì‹ ë“± ì¶”ê°€ ëŒ€ì‘ ê°€ëŠ¥
    try:
        return datetime.strptime(date_str, "%Y-%m-%d %H:%M")
    except:
        pass
        
    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜í•˜ê±°ë‚˜ None (ì—¬ê¸°ì„  ì•ˆì „í•˜ê²Œ ì•„ì£¼ ë¨¼ ê³¼ê±°)
    return datetime.min

def run_crawler(script_path: Path, cwd: Path):
    """í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
    print(f"ğŸš€ Running crawler: {script_path.name}...")
    try:
        subprocess.run(
            ["python3", str(script_path)], 
            cwd=str(cwd), 
            check=True,
            capture_output=False 
        )
        print(f"âœ… Crawler finished: {script_path.name}")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Crawler failed: {e}")

def load_json_data(file_path: Path) -> List[Dict]:
    if not file_path.exists():
        print(f"âš ï¸ File not found: {file_path}")
        return []
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        
        # ë‰´ìŠ¤ ë°ì´í„° êµ¬ì¡° (categories -> articles)
        if 'categories' in data:
            all_items = []
            for cat in data['categories']:
                articles = cat.get('articles', []) or cat.get('videos', [])
                for item in articles:
                    item['source_category'] = cat.get('main_category', 'Unknown') or cat.get('category_name', 'Unknown')
                    all_items.append(item)
            return all_items
            
        return data

def filter_by_date(items: List[Dict], start_dt: datetime, end_dt: datetime, type: str) -> List[Dict]:
    filtered = []
    for item in items:
        item_dt = datetime.min
        
        if type == 'news':
            date_str = item.get('published_time', '')
            if date_str:
                item_dt = parse_korean_datetime(date_str)
        elif type == 'youtube':
            # ìœ íŠœë¸ŒëŠ” ì •í™•í•œ ê²Œì‹œì¼ ì¶”ì¶œì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ. 
            # fetched_atì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ê±°ë‚˜, í…ìŠ¤íŠ¸ íŒŒì‹± í•„ìš”.
            # ì—¬ê¸°ì„  fetched_at(í¬ë¡¤ë§ ì‹œì )ì´ ë²”ìœ„ ë‚´ì¸ì§€, í˜¹ì€ ì˜ìƒ ë‚´ ë‚ ì§œ í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒ ìš°ì„ 
            fetched_at = item.get('fetched_at', '')
            if fetched_at:
                try:
                    item_dt = datetime.fromisoformat(fetched_at)
                except:
                    pass
        
        # ë²”ìœ„ ì²´í¬
        if start_dt <= item_dt <= end_dt:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ (ì •ë ¬ìš©)
            item['timestamp_obj'] = item_dt.isoformat()
            filtered.append(item)
            
    return filtered

def categorize_item(item: Dict, trends: Dict[str, float]) -> str:
    """
    ì•„ì´í…œì„ IT ì „ìš© 5ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
    - Tech: í…Œí¬ì‚°ì—… (ê¸°ì—… ë™í–¥, M&A, ì‹œì¥, ìŠ¤íƒ€íŠ¸ì—…)
    - AI: ì¸ê³µì§€ëŠ¥ (AI, ML, LLM, ìƒì„±í˜•AI, ë¡œë³´í‹±ìŠ¤)
    - Dev: ê°œë°œ (í”„ë ˆì„ì›Œí¬, ì–¸ì–´, ì˜¤í”ˆì†ŒìŠ¤, DevOps)
    - Product: ì„œë¹„ìŠ¤ (ì‹ ê·œ ì„œë¹„ìŠ¤, ì•±, í”Œë«í¼, UX)
    - Security: ë³´ì•ˆ (ì‚¬ì´ë²„ ë³´ì•ˆ, í´ë¼ìš°ë“œ, ì¸í”„ë¼)
    """
    text = (item.get('title', '') + " " + item.get('content', '')).lower()

    # AI ê´€ë ¨ (ê°€ì¥ ë¨¼ì € ì²´í¬ - ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì™€ ê²¹ì¹  ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
    if any(k in text for k in ['ì¸ê³µì§€ëŠ¥', 'ai ', ' ai', 'llm', 'gpt', 'gemini', 'claude',
                                'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ìƒì„±í˜•', 'ì±—ë´‡', 'openai', 'ì–¸ì–´ëª¨ë¸',
                                'diffusion', 'transformer', 'ë¡œë³´í‹±ìŠ¤', 'ììœ¨ì£¼í–‰',
                                'ì‹ ê²½ë§', 'sora', 'copilot', 'íŒŒì¸íŠœë‹', 'rag']):
        return 'AI'

    # ë³´ì•ˆ/ì¸í”„ë¼
    if any(k in text for k in ['ë³´ì•ˆ', 'í•´í‚¹', 'ì·¨ì•½ì ', 'ëœì„¬ì›¨ì–´', 'ê°œì¸ì •ë³´',
                                'í´ë¼ìš°ë“œ', 'aws', 'azure', 'gcp', 'ë°ì´í„°ì„¼í„°',
                                'ì‚¬ì´ë²„', 'í”¼ì‹±', 'ddos', 'ì¸í”„ë¼', 'ì„œë²„',
                                'zero-day', 'ì•”í˜¸í™”', 'kubernetes', 'k8s']):
        return 'Security'

    # ê°œë°œ
    if any(k in text for k in ['ê°œë°œì', 'í”„ë ˆì„ì›Œí¬', 'ì˜¤í”ˆì†ŒìŠ¤', 'github', 'devops',
                                'python', 'javascript', 'typescript', 'rust', 'golang',
                                'react', 'next.js', 'docker', 'api', 'sdk',
                                'ë¼ì´ë¸ŒëŸ¬ë¦¬', 'í”„ë¡œê·¸ë˜ë°', 'ì½”ë”©', 'ì»¨í…Œì´ë„ˆ',
                                'ci/cd', 'git', 'vscode', 'ê°œë°œ ë„êµ¬', 'ë¦´ë¦¬ìŠ¤']):
        return 'Dev'

    # ì„œë¹„ìŠ¤/í”„ë¡œë•íŠ¸
    if any(k in text for k in ['ì¶œì‹œ', 'ì—…ë°ì´íŠ¸', 'ì„œë¹„ìŠ¤', 'í”Œë«í¼', 'ì‚¬ìš©ì',
                                'êµ¬ë…', 'ux', 'ui', 'ì•±ìŠ¤í† ì–´', 'ë‹¤ìš´ë¡œë“œ',
                                'ë² íƒ€', 'ëŸ°ì¹­', 'ì‹ ê·œ ê¸°ëŠ¥', 'ê°€ì…ì',
                                'ì¹´ì¹´ì˜¤', 'ë„¤ì´ë²„', 'í† ìŠ¤', 'ë‹¹ê·¼', 'ë°°ë¯¼']):
        return 'Product'

    # í…Œí¬ì‚°ì—… (ê¸°ë³¸ IT ì¹´í…Œê³ ë¦¬)
    return 'Tech'


def is_it_content(item: Dict) -> bool:
    """IT/í…Œí¬ ê´€ë ¨ ì½˜í…ì¸ ì¸ì§€ í•„í„°ë§ (ë¹„IT ì½˜í…ì¸  ì œê±°ìš©)"""
    text = (item.get('title', '') + " " + item.get('content', '')).lower()

    # ë¹„IT ì½˜í…ì¸  í‚¤ì›Œë“œ (ê²½ì œ, ì¬í…Œí¬, ì‚¬íšŒ ë“±)
    non_it_keywords = [
        # ê²½ì œ/ì¬í…Œí¬
        'ì£¼ì‹', 'ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'í™˜ìœ¨', 'ê¸ˆë¦¬', 'ë¶€ë™ì‚°', 'ì•„íŒŒíŠ¸', 'ì²­ì•½',
        'ì ê¸ˆ', 'í€ë“œ', 'ì±„ê¶Œ', 'etf', 'ë°°ë‹¹', 'ì¦ì‹œ', 'ì½”ì¸', 'ë¹„íŠ¸ì½”ì¸',
        'íˆ¬ì', 'ì¬í…Œí¬', 'ì—°ê¸ˆ', 'ëŒ€ì¶œ', 'ì˜ˆê¸ˆ',
        # ì‚¬íšŒ/ì •ì¹˜
        'ì‚¬ê±´', 'ì‚¬ê³ ', 'ë‚ ì”¨', 'êµí†µ', 'ì •ì¹˜', 'ì„ ê±°', 'êµ­íšŒ', 'ëŒ€í†µë ¹',
        'ì¬íŒ', 'ê²€ì°°', 'ê²½ì°°', 'ë²”ì£„', 'ì‚¬ë§', 'í™”ì¬',
    ]

    # IT ê´€ë ¨ í‚¤ì›Œë“œ
    it_keywords = [
        'it', 'í…Œí¬', 'ê¸°ìˆ ', 'ì†Œí”„íŠ¸ì›¨ì–´', 'í•˜ë“œì›¨ì–´', 'ë°˜ë„ì²´', 'ì¹©',
        'ai', 'ì¸ê³µì§€ëŠ¥', 'í´ë¼ìš°ë“œ', 'ë°ì´í„°', 'ì„œë²„', 'ê°œë°œ',
        'ì•±', 'í”Œë«í¼', 'ìŠ¤íƒ€íŠ¸ì—…', 'ë¹…í…Œí¬', 'êµ¬ê¸€', 'ì• í”Œ', 'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸',
        'ë©”íƒ€', 'ì•„ë§ˆì¡´', 'ì—”ë¹„ë””ì•„', 'tsmc', 'ì‚¼ì„±ì „ì', 'skí•˜ì´ë‹‰ìŠ¤',
        'ë„¤ì´ë²„', 'ì¹´ì¹´ì˜¤', 'ë¼ì¸', 'ì¿ íŒ¡', 'ë°°ë‹¬ì˜ë¯¼ì¡±', 'í† ìŠ¤',
        'ë³´ì•ˆ', 'í•´í‚¹', 'ì˜¤í”ˆì†ŒìŠ¤', 'api', 'ë¡œë´‡', 'ììœ¨ì£¼í–‰',
        'ë¸”ë¡ì²´ì¸', 'ë©”íƒ€ë²„ìŠ¤', 'vr', 'ar', 'xr', 'ì›¨ì–´ëŸ¬ë¸”',
        '5g', '6g', 'í†µì‹ ', 'ë„¤íŠ¸ì›Œí¬', 'ì‚¬ë¬¼ì¸í„°ë„·', 'iot',
        'saas', 'paas', 'ë””ì§€í„¸', 'íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜',
    ]

    # IT í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ì²´í¬
    has_it = any(k in text for k in it_keywords)

    # ë¹„ITë§Œ í¬í•¨í•˜ê³  ITëŠ” ì—†ëŠ” ê²½ìš° ì œì™¸
    has_non_it_only = any(k in text for k in non_it_keywords) and not has_it

    return has_it or not has_non_it_only

def main():
    args = parse_args()
    
    try:
        start_dt = datetime.strptime(args.start, "%Y-%m-%d %H:%M")
        end_dt = datetime.strptime(args.end, "%Y-%m-%d %H:%M")
    except ValueError:
        print("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYY-MM-DD HH:MM í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
        
    print(f"ğŸ—“ï¸ Target Period: {start_dt} ~ {end_dt}")
    
    base_dir = Path(__file__).resolve().parent.parent
    
    # 1. Crawl (if not skipped)
    if not args.skip_crawl:
        # Naver News
        news_script = base_dir / "crawling_naver_news" / "news_crawler.py"
        run_crawler(news_script, news_script.parent)
        
        # Youtube (Data API v3 â€” AWS IP ì°¨ë‹¨ ìš°íšŒ)
        youtube_script = base_dir / "crawling_youtube" / "youtube_crawler_api.py"
        run_crawler(youtube_script, youtube_script.parent)
    
    # 2. Load Data
    news_file = base_dir / "crawling_naver_news" / "news_data.json"
    youtube_file = base_dir / "crawling_youtube" / "youtube_data.json"
    
    news_items = load_json_data(news_file)
    youtube_items = load_json_data(youtube_file)
    
    print(f"\nğŸ“¥ Loaded: {len(news_items)} news, {len(youtube_items)} videos")
    
    # 3. Filter by Date
    filtered_news = filter_by_date(news_items, start_dt, end_dt, 'news')
    filtered_youtube = filter_by_date(youtube_items, start_dt, end_dt, 'youtube')
    
    print(f"ğŸ“‰ Filtered (Date): {len(filtered_news)} news, {len(filtered_youtube)} videos")
    
    if not filtered_news and not filtered_youtube:
        print("âš ï¸ ê¸°ê°„ ë‚´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        # ë°ì´í„°ê°€ ì—†ì–´ë„ íŠ¸ë Œë“œëŠ” ë½‘ì•„ì„œ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ
    
    # 4. Get Trends & Score
    print("\nğŸ“Š Collecting Trends...")
    collector = TrendCollector()
    
    trends_map = {}
    # Google
    for k in collector.get_google_trends(): trends_map[k] = trends_map.get(k, 0) + 1.5
    # Naver
    for k in collector.get_naver_datalab_trends(): trends_map[k] = trends_map.get(k, 0) + 1.2
    # BlackKiwi
    bk = collector.get_blackkiwi_trends()
    for k in bk.get('rising', []): trends_map[k] = trends_map.get(k, 0) + 1.8
    for k in bk.get('new', []): trends_map[k] = trends_map.get(k, 0) + 1.5
    
    print(f"   Collected {len(trends_map)} trend keywords.")
    
    # Scoring Combined List
    all_content = []
    
    # News Scoring
    for item in filtered_news:
        score = 0
        matched = []
        text = (item.get('title', '') + " " + item.get('content', '')).lower()
        for kw, weight in trends_map.items():
            if kw.lower() in text:
                score += weight
                matched.append(kw)
        
        item['trend_score'] = score
        item['matched_keywords'] = matched
        item['type'] = 'news'
        all_content.append(item)
        
    # Youtube Scoring
    for item in filtered_youtube:
        score = 0
        matched = []
        text = (item.get('title', '') + " " + item.get('description', '') + " " + item.get('search_keyword', '')).lower()
        if item.get('transcript'): # ìë§‰ ìˆìœ¼ë©´ ìë§‰ë„ ê²€ìƒ‰
             text += " " + item['transcript'].get('full_text', '')[:1000] # ì•ë¶€ë¶„ë§Œ

        for kw, weight in trends_map.items():
            if kw.lower() in text:
                score += weight * 1.5 # ìœ íŠœë¸ŒëŠ” ì˜ìƒì´ë¼ ê°€ì¤‘ì¹˜ ì¡°ê¸ˆ ë” ì¤Œ (ì„ íƒ)
                matched.append(kw)
        
        item['trend_score'] = score
        item['matched_keywords'] = matched
        item['type'] = 'youtube'
        all_content.append(item)
    
    # 4.5. IT ì½˜í…ì¸  í•„í„°ë§ (ë¹„IT ì½˜í…ì¸  ì œê±°)
    it_content = [item for item in all_content if is_it_content(item)]
    filtered_out = len(all_content) - len(it_content)
    if filtered_out > 0:
        print(f"ğŸš« ë¹„IT ì½˜í…ì¸  {filtered_out}ê±´ ì œê±°ë¨")
    all_content = it_content

    # 5. Categorize & Sort (IT ì „ìš© 5ê°œ ì¹´í…Œê³ ë¦¬)
    final_report = {
        "generated_at": datetime.now().isoformat(),
        "period": {"start": args.start, "end": args.end},
        "trends_summary": sorted(trends_map.keys(), key=lambda k: trends_map[k], reverse=True)[:10],
        "categories": {
            "Tech": [],
            "AI": [],
            "Dev": [],
            "Product": [],
            "Security": []
        }
    }

    # Sort by score descending
    all_content.sort(key=lambda x: x['trend_score'], reverse=True)

    for item in all_content:
        cat = categorize_item(item, trends_map)
        final_report["categories"][cat].append(item)
    
    # 6. Save Report
    output_filename = f"daily_brief_{start_dt.strftime('%Y%m%d')}.json"
    output_dir = base_dir # pipeline folder
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / output_filename
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)
        
    print("\n" + "="*60)
    print(f"âœ… Daily Brief Generated: {output_path}")
    print(f"   - Trends: {len(trends_map)}")
    print(f"   - Tech (í…Œí¬ì‚°ì—…): {len(final_report['categories']['Tech'])}")
    print(f"   - AI: {len(final_report['categories']['AI'])}")
    print(f"   - Dev (ê°œë°œ): {len(final_report['categories']['Dev'])}")
    print(f"   - Product (ì„œë¹„ìŠ¤): {len(final_report['categories']['Product'])}")
    print(f"   - Security (ë³´ì•ˆ): {len(final_report['categories']['Security'])}")
    print("="*60)

if __name__ == "__main__":
    main()
