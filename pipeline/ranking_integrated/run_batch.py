import argparse
import json
import os
import sys
import subprocess
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€í•˜ì—¬ generate_briefing ëª¨ë“ˆ import
current_dir = Path(__file__).resolve().parent
sys.path.append(str(current_dir))

try:
    from generate_briefing import TrendCollector, BriefingGenerator
except ImportError as e:
    print(f"âŒ generate_briefing ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    sys.exit(1)

def parse_args():
    parser = argparse.ArgumentParser(description='Daily Trend Briefing Batch Job')
    parser.add_argument('--start', type=str, required=True, help='Start DateTime (YYYY-MM-DD HH:MM)')
    parser.add_argument('--end', type=str, required=True, help='End DateTime (YYYY-MM-DD HH:MM)')
    parser.add_argument('--skip-crawl', action='store_true', help='Skip crawling and use existing data')
    return parser.parse_args()

def parse_korean_datetime(date_str: str) -> datetime:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ë“±ì˜ ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±
    ì˜ˆ: '2024.02.02. ì˜¤ì „ 10:30', '1ì‹œê°„ ì „', '5ë¶„ ì „'
    """
    now = datetime.now()
    date_str = date_str.strip()
    
    # 1. "Në¶„ ì „", "Nì‹œê°„ ì „", "Nì¼ ì „" ì²˜ë¦¬ (ê³µë°± ìœ ë¬´ ìƒê´€ì—†ì´)
    if 'ë¶„' in date_str and 'ì „' in date_str:
        try:
            minutes = int(re.search(r'(\d+)', date_str).group(1))
            return now - timedelta(minutes=minutes)
        except:
            pass
    elif 'ì‹œê°„' in date_str and 'ì „' in date_str:
        try:
            hours = int(re.search(r'(\d+)', date_str).group(1))
            return now - timedelta(hours=hours)
        except:
            pass
    elif 'ì¼' in date_str and 'ì „' in date_str:
        try:
            days = int(re.search(r'(\d+)', date_str).group(1))
            return now - timedelta(days=days)
        except:
            pass
    
    # 2. "2024.02.02. ì˜¤ì „ 10:30" í˜•ì‹ ì²˜ë¦¬
    try:
        # 'ì˜¤ì „', 'ì˜¤í›„' ì²˜ë¦¬
        is_pm = 'ì˜¤í›„' in date_str
        cleaned = re.sub(r'(ì˜¤ì „|ì˜¤í›„)\s*', '', date_str).strip()
        # ì (.)ì´ ë§ˆì§€ë§‰ì— ìˆì„ ìˆ˜ ìˆìŒ
        cleaned = cleaned.rstrip('.')
        
        # í¬ë§· ë§ì¶”ê¸° (YYYY.MM.DD HH:MM)
        dt = datetime.strptime(cleaned, "%Y.%m.%d. %H:%M")
        
        if is_pm and dt.hour < 12:
            dt = dt + timedelta(hours=12)
        elif not is_pm and dt.hour == 12: # ì˜¤ì „ 12ì‹œëŠ” 0ì‹œ
            dt = dt - timedelta(hours=12)
            
        return dt
    except:
        pass

    # 3. YYYY-MM-DD í˜•ì‹ ë“± ì¶”ê°€ ëŒ€ì‘ ê°€ëŠ¥
    try:
        return datetime.strptime(date_str, "%Y-%m-%d %H:%M")
    except:
        pass
        
    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜í•˜ê±°ë‚˜ None (ì—¬ê¸°ì„  ì•ˆì „í•˜ê²Œ ì•„ì£¼ ë¨¼ ê³¼ê±°)
    return datetime.min

def run_crawler(script_path: Path, cwd: Path):
    """í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
    print(f"ğŸš€ Running crawler: {script_path.name}...")
    try:
        subprocess.run(
            ["python3", str(script_path)], 
            cwd=str(cwd), 
            check=True,
            capture_output=False 
        )
        print(f"âœ… Crawler finished: {script_path.name}")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Crawler failed: {e}")

def load_json_data(file_path: Path) -> List[Dict]:
    if not file_path.exists():
        print(f"âš ï¸ File not found: {file_path}")
        return []
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        
        # ë‰´ìŠ¤ ë°ì´í„° êµ¬ì¡° (categories -> articles)
        if 'categories' in data:
            all_items = []
            for cat in data['categories']:
                articles = cat.get('articles', []) or cat.get('videos', [])
                for item in articles:
                    item['source_category'] = cat.get('main_category', 'Unknown') or cat.get('category_name', 'Unknown')
                    all_items.append(item)
            return all_items
            
        return data

def filter_by_date(items: List[Dict], start_dt: datetime, end_dt: datetime, type: str) -> List[Dict]:
    filtered = []
    for item in items:
        item_dt = datetime.min
        
        if type == 'news':
            date_str = item.get('published_time', '')
            if date_str:
                item_dt = parse_korean_datetime(date_str)
        elif type == 'youtube':
            # ìœ íŠœë¸ŒëŠ” ì •í™•í•œ ê²Œì‹œì¼ ì¶”ì¶œì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ. 
            # fetched_atì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ê±°ë‚˜, í…ìŠ¤íŠ¸ íŒŒì‹± í•„ìš”.
            # ì—¬ê¸°ì„  fetched_at(í¬ë¡¤ë§ ì‹œì )ì´ ë²”ìœ„ ë‚´ì¸ì§€, í˜¹ì€ ì˜ìƒ ë‚´ ë‚ ì§œ í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒ ìš°ì„ 
            fetched_at = item.get('fetched_at', '')
            if fetched_at:
                try:
                    item_dt = datetime.fromisoformat(fetched_at)
                except:
                    pass
        
        # ë²”ìœ„ ì²´í¬
        if start_dt <= item_dt <= end_dt:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ (ì •ë ¬ìš©)
            item['timestamp_obj'] = item_dt.isoformat()
            filtered.append(item)
            
    return filtered

def categorize_item(item: Dict, trends: Dict[str, float]) -> str:
    """
    ì•„ì´í…œì„ 4ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ (Economy, Society, Money, Trend)
    """
    title = item.get('title', '') + " " + item.get('content', '')
    
    # 1. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ í™•ì¸
    # (ì´ë¯¸ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ë˜ì–´ìˆë‹¤ë©´ ê·¸ ì •ë³´ í™œìš© ê°€ëŠ¥í•˜ì§€ë§Œ, ì—¬ê¸°ì„  ë…ë¦½ì ìœ¼ë¡œ)
    
    # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
    title_lower = title.lower()
    
    if any(k in title_lower for k in ['ì£¼ì‹', 'íˆ¬ì', 'ì½”ì¸', 'ë¹„íŠ¸ì½”ì¸', 'ë¶€ë™ì‚°', 'ì²­ì•½', 'ì‚¼ì„±ì „ì', 'ì ê¸ˆ']):
        return 'Money' # ì¬í…Œí¬
    
    if any(k in title_lower for k in ['ê²½ì œ', 'ê¸ˆë¦¬', 'ìˆ˜ì¶œ', 'GDP', 'í™˜ìœ¨', 'ê¸°ì—…']):
        return 'Economy' # ê²½ì œ
        
    if any(k in title_lower for k in ['ì‚¬íšŒ', 'ì‚¬ê±´', 'ì‚¬ê³ ', 'ë‚ ì”¨', 'êµí†µ', 'ì •ì¹˜']):
        return 'Society' # ì‚¬íšŒ
        
    # ê·¸ ì™¸ëŠ” íŠ¸ë Œë“œì„±ì´ê±°ë‚˜ ê¸°íƒ€
    return 'Trend'

def main():
    args = parse_args()
    
    try:
        start_dt = datetime.strptime(args.start, "%Y-%m-%d %H:%M")
        end_dt = datetime.strptime(args.end, "%Y-%m-%d %H:%M")
    except ValueError:
        print("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYY-MM-DD HH:MM í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
        
    print(f"ğŸ—“ï¸ Target Period: {start_dt} ~ {end_dt}")
    
    base_dir = Path(__file__).resolve().parent.parent
    
    # 1. Crawl (if not skipped)
    if not args.skip_crawl:
        # Naver News
        news_script = base_dir / "crawling_naver_news" / "news_crawler.py"
        run_crawler(news_script, news_script.parent)
        
        # Youtube (Data API v3 â€” AWS IP ì°¨ë‹¨ ìš°íšŒ)
        youtube_script = base_dir / "crawling_youtube" / "youtube_crawler_api.py"
        run_crawler(youtube_script, youtube_script.parent)
    
    # 2. Load Data
    news_file = base_dir / "crawling_naver_news" / "news_data.json"
    youtube_file = base_dir / "crawling_youtube" / "youtube_data.json"
    
    news_items = load_json_data(news_file)
    youtube_items = load_json_data(youtube_file)
    
    print(f"\nğŸ“¥ Loaded: {len(news_items)} news, {len(youtube_items)} videos")
    
    # 3. Filter by Date
    filtered_news = filter_by_date(news_items, start_dt, end_dt, 'news')
    filtered_youtube = filter_by_date(youtube_items, start_dt, end_dt, 'youtube')
    
    print(f"ğŸ“‰ Filtered (Date): {len(filtered_news)} news, {len(filtered_youtube)} videos")
    
    if not filtered_news and not filtered_youtube:
        print("âš ï¸ ê¸°ê°„ ë‚´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        # ë°ì´í„°ê°€ ì—†ì–´ë„ íŠ¸ë Œë“œëŠ” ë½‘ì•„ì„œ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ
    
    # 4. Get Trends & Score
    print("\nğŸ“Š Collecting Trends...")
    collector = TrendCollector()
    
    trends_map = {}
    # Google
    for k in collector.get_google_trends(): trends_map[k] = trends_map.get(k, 0) + 1.5
    # Naver
    for k in collector.get_naver_datalab_trends(): trends_map[k] = trends_map.get(k, 0) + 1.2
    # BlackKiwi
    bk = collector.get_blackkiwi_trends()
    for k in bk.get('rising', []): trends_map[k] = trends_map.get(k, 0) + 1.8
    for k in bk.get('new', []): trends_map[k] = trends_map.get(k, 0) + 1.5
    
    print(f"   Collected {len(trends_map)} trend keywords.")
    
    # Scoring Combined List
    all_content = []
    
    # News Scoring
    for item in filtered_news:
        score = 0
        matched = []
        text = (item.get('title', '') + " " + item.get('content', '')).lower()
        for kw, weight in trends_map.items():
            if kw.lower() in text:
                score += weight
                matched.append(kw)
        
        item['trend_score'] = score
        item['matched_keywords'] = matched
        item['type'] = 'news'
        all_content.append(item)
        
    # Youtube Scoring
    for item in filtered_youtube:
        score = 0
        matched = []
        text = (item.get('title', '') + " " + item.get('description', '') + " " + item.get('search_keyword', '')).lower()
        if item.get('transcript'): # ìë§‰ ìˆìœ¼ë©´ ìë§‰ë„ ê²€ìƒ‰
             text += " " + item['transcript'].get('full_text', '')[:1000] # ì•ë¶€ë¶„ë§Œ

        for kw, weight in trends_map.items():
            if kw.lower() in text:
                score += weight * 1.5 # ìœ íŠœë¸ŒëŠ” ì˜ìƒì´ë¼ ê°€ì¤‘ì¹˜ ì¡°ê¸ˆ ë” ì¤Œ (ì„ íƒ)
                matched.append(kw)
        
        item['trend_score'] = score
        item['matched_keywords'] = matched
        item['type'] = 'youtube'
        all_content.append(item)
    
    # 5. Categorize & Sort
    final_report = {
        "generated_at": datetime.now().isoformat(),
        "period": {"start": args.start, "end": args.end},
        "trends_summary": sorted(trends_map.keys(), key=lambda k: trends_map[k], reverse=True)[:10],
        "categories": {
            "Economy": [],
            "Money": [],
            "Society": [],
            "Trend": []
        }
    }
    
    # Sort by score descending
    all_content.sort(key=lambda x: x['trend_score'], reverse=True)
    
    for item in all_content:
        cat = categorize_item(item, trends_map)
        final_report["categories"][cat].append(item)
    
    # 6. Save Report
    output_filename = f"daily_brief_{start_dt.strftime('%Y%m%d')}.json"
    output_dir = base_dir # pipeline folder
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / output_filename
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)
        
    print("\n" + "="*60)
    print(f"âœ… Daily Brief Generated: {output_path}")
    print(f"   - Trends: {len(trends_map)}")
    print(f"   - Economy: {len(final_report['categories']['Economy'])}")
    print(f"   - Money: {len(final_report['categories']['Money'])}")
    print(f"   - Society: {len(final_report['categories']['Society'])}")
    print(f"   - Trend: {len(final_report['categories']['Trend'])}")
    print("="*60)

if __name__ == "__main__":
    main()
