import sys
import os
import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Set

# ìƒìœ„ í´ë”(ë£¨íŠ¸)ë¥¼ ê²½ë¡œì— ì¶”ê°€í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“ˆ import ê°€ëŠ¥í•˜ê²Œ í•¨
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ê° íŠ¸ë Œë“œ ëª¨ë“ˆ import
# (ì£¼ì˜: ê° íŒŒì¼ì´ ëª¨ë“ˆë¡œì„œ import ê°€ëŠ¥í•œ êµ¬ì¡°ì—¬ì•¼ í•¨. í•¨ìˆ˜ë“¤ì´ if __name__ == "__main__": ë¸”ë¡ ë°–ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨)
# ì¼ë‹¨ ì—¬ê¸°ì„œëŠ” ì§ì ‘ ë¡œì§ì„ import í•˜ê±°ë‚˜, ê° íŒŒì¼ì˜ í•µì‹¬ í•¨ìˆ˜ë¥¼ ë³µì‚¬í•´ì™€ì„œ í†µí•© í´ë˜ìŠ¤ë¡œ êµ¬ì„±í•  ì˜ˆì •.
# í•˜ì§€ë§Œ ê¸°ì¡´ íŒŒì¼ë“¤ì„ ìˆ˜ì •í•˜ì§€ ì•Šê³  import í•˜ë ¤ë©´ í•´ë‹¹ íŒŒì¼ë“¤ì˜ êµ¬ì¡°ê°€ ì¤‘ìš”í•¨.
# í¸ì˜ìƒ í•µì‹¬ ë¡œì§ì„ ì´ íŒŒì¼ì—ì„œ ì§ì ‘ í˜¸ì¶œí•˜ê±°ë‚˜, subprocessë¡œ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë§Œ íŒŒì‹±í•˜ëŠ” ë°©ë²•ë„ ìˆìŒ.
# ê°€ì¥ ê¹”ë”í•œ ê±´ ì—¬ê¸°ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì²˜ëŸ¼ ì“°ëŠ” ê²ƒ.

try:
    # Google Trends
    from ranking_google_trends.check_google_trends_rss import *
    # BlackKiwi
    from ranking_blackkiwi.check_blackkiwi_trend import *
except ImportError:
    pass

# =========================================================================================
# 1. íŠ¸ë Œë“œ ìˆ˜ì§‘ í´ë˜ìŠ¤
# =========================================================================================
import requests
import xml.etree.ElementTree as ET
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from dotenv import load_dotenv

class TrendCollector:
    def __init__(self):
        self.load_env()
        
    def load_env(self):
        # pipeline/ranking_integrated/generate_briefing.py -> pipeline -> project root -> .env
        env_path = Path(__file__).resolve().parent.parent.parent / ".env"
        load_dotenv(dotenv_path=env_path)
        self.naver_client_id = os.getenv("NAVER_CLIENT_ID")
        self.naver_client_secret = os.getenv("NAVER_CLIENT_SECRET")

    def get_google_trends(self) -> List[str]:
        """êµ¬ê¸€ íŠ¸ë Œë“œ RSSì—ì„œ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ì¶”ì¶œ"""
        print("   [Google] íŠ¸ë Œë“œ ìˆ˜ì§‘ ì¤‘...")
        url = "https://trends.google.co.kr/trending/rss?geo=KR"
        keywords = []
        try:
            resp = requests.get(url, timeout=5)
            if resp.status_code == 200:
                root = ET.fromstring(resp.content)
                for item in root.findall('.//item'):
                    title = item.find('title').text
                    keywords.append(title)
        except Exception as e:
            print(f"      âŒ Google ì˜¤ë¥˜: {e}")
        return keywords

    def get_naver_datalab_trends(self) -> List[str]:
        """ë„¤ì´ë²„ ë°ì´í„°ë© APIì—ì„œ ê²½ì œ/ì£¼ì‹/ì‚¬íšŒ íŠ¸ë Œë“œ ì¶”ì¶œ"""
        print("   [Naver] ë°ì´í„°ë© íŠ¸ë Œë“œ ìˆ˜ì§‘ ì¤‘...")
        if not self.naver_client_id or not self.naver_client_secret:
            print("      âš ï¸ ë„¤ì´ë²„ API í‚¤ ì—†ìŒ")
            return []
            
        url = "https://openapi.naver.com/v1/datalab/search"
        headers = {
            "X-Naver-Client-Id": self.naver_client_id,
            "X-Naver-Client-Secret": self.naver_client_secret,
            "Content-Type": "application/json",
        }
        
        # ì¡°íšŒí•  í‚¤ì›Œë“œ ê·¸ë£¹ (ê²½ì œ ìœ„ì£¼)
        groups = [
            {"groupName": "ê²½ì œ", "keywords": ["ê²½ì œ", "ê¸ˆë¦¬", "í™˜ìœ¨", "ë¬¼ê°€"]},
            {"groupName": "ì£¼ì‹", "keywords": ["ì£¼ì‹", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ìƒì¥"]},
            {"groupName": "ë¶€ë™ì‚°", "keywords": ["ë¶€ë™ì‚°", "ì•„íŒŒíŠ¸", "ì²­ì•½", "ì „ì„¸"]},
             {"groupName": "ë°˜ë„ì²´/AI", "keywords": ["ë°˜ë„ì²´", "AI", "ì¸ê³µì§€ëŠ¥", "ì‚¼ì„±ì „ì", "í•˜ì´ë‹‰ìŠ¤"]},
        ]
        
        end_date = datetime.now().date()
        start_date = end_date  # ì˜¤ëŠ˜ ë°ì´í„° ì¡°íšŒ ì‹œë„ (ì—†ìœ¼ë©´ ì–´ì œêº¼ ì”€ ë“± ë¡œì§ í•„ìš”í•˜ì§€ë§Œ ì¼ë‹¨ ì˜¤ëŠ˜)
        # ë°ì´í„°ë©ì€ í•˜ë£¨ ì „ ë°ì´í„°ê¹Œì§€ë§Œ ë³´í†µ ì •í™•íˆ ë‚˜ì˜´
        from datetime import timedelta
        start_date = end_date - timedelta(days=1)
        
        body = {
            "startDate": start_date.strftime("%Y-%m-%d"),
            "endDate": end_date.strftime("%Y-%m-%d"),
            "timeUnit": "date",
            "keywordGroups": groups
        }
        
        keywords = []
        try:
            resp = requests.post(url, headers=headers, json=body, timeout=5)
            if resp.json().get('results'):
                # ratioê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ê±°ë‚˜, ê·¸ëƒ¥ ê·¸ë£¹ëª… ìì²´ë¥¼ í‚¤ì›Œë“œë¡œ í™œìš©
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ê²°ê³¼ì— ìˆëŠ” ê·¸ë£¹ëª…ë§Œ ê°€ì ¸ì˜´
                for result in resp.json()['results']:
                     # ratio í™•ì¸
                    data = result.get('data', [])
                    if data:
                        last_ratio = data[-1].get('ratio', 0)
                        if last_ratio > 10: # ì˜ë¯¸ìˆëŠ” ê²€ìƒ‰ëŸ‰ì´ ìˆì„ ë•Œë§Œ
                            keywords.append(result['title']) 
                            # ìƒì„¸ í‚¤ì›Œë“œë„ ì¶”ê°€í•˜ë©´ ì¢‹ìŒ
        except Exception as e:
            print(f"      âŒ Naver ì˜¤ë¥˜: {e}")
            
        return keywords

    def get_blackkiwi_trends(self) -> Dict[str, List[str]]:
        """ë¸”ë™í‚¤ìœ„ì—ì„œ ê¸‰ìƒìŠ¹/ì‹ ê·œ í‚¤ì›Œë“œ í¬ë¡¤ë§ (Selenium)"""
        print("   [BlackKiwi] íŠ¸ë Œë“œ ìˆ˜ì§‘ ì¤‘ (ë¸Œë¼ìš°ì € ì‹¤í–‰)...")
        url = "https://blackkiwi.net/service/trend"
        results = {"rising": [], "new": []}
        
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        driver = None
        try:
            driver = webdriver.Chrome(options=options)
            driver.get(url)
            wait = WebDriverWait(driver, 10)
            
            # Rising
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#popularKeywordList a")))
                elems = driver.find_elements(By.CSS_SELECTOR, "#popularKeywordList a")
                results["rising"] = [e.text.strip() for e in elems if e.text.strip()][:10]
            except: pass
            
            # New
            try:
                elems = driver.find_elements(By.CSS_SELECTOR, "div[class*='NewKeywordsList'] a")
                results["new"] = [e.text.strip() for e in elems if e.text.strip()][:10]
            except: pass
            
        except Exception as e:
            print(f"      âŒ BlackKiwi ì˜¤ë¥˜: {e}")
        finally:
            if driver:
                driver.quit()
                
        return results

# =========================================================================================
# 2. ë‰´ìŠ¤ ë§¤ì¹­ í´ë˜ìŠ¤
# =========================================================================================
class BriefingGenerator:
    def __init__(self, news_json_path: str):
        self.news_data = self.load_news(news_json_path)
        
    def load_news(self, path: str):
        if not os.path.exists(path):
            print(f"âš ï¸ ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}")
            return []
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # dataê°€ categoryë³„ë¡œ ë¬¶ì—¬ìˆë‹¤ë©´ flatí•˜ê²Œ í’€ì–´ì„œ ê²€ìƒ‰í•˜ê¸° ì‰½ê²Œ ë§Œë“¦
            all_articles = []
            if 'categories' in data:
                for cat in data['categories']:
                    all_articles.extend(cat.get('articles', []))
            return all_articles

    def match_trends(self, trend_keywords: Dict[str, float]) -> List[Dict]:
        """
        íŠ¸ë Œë“œ í‚¤ì›Œë“œ(ê°€ì¤‘ì¹˜ í¬í•¨)ì™€ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë§¤ì¹­
        trend_keywords: {'í‚¤ì›Œë“œ': ê°€ì¤‘ì¹˜ì ìˆ˜}
        """
        matched_results = []
        
        # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ë©° í‚¤ì›Œë“œ ê²€ìƒ‰
        # (ë‹¨ìˆœ ë¬¸ìì—´ í¬í•¨ ì—¬ë¶€ ì²´í¬)
        
        for article in self.news_data:
            title = article.get('title', '')
            content = article.get('content', '')
            score = 0
            matched_kws = []
            
            for rank_kw, weight in trend_keywords.items():
                # í‚¤ì›Œë“œê°€ ì œëª©ì´ë‚˜ ë‚´ìš©ì— ìˆëŠ”ì§€ í™•ì¸
                if rank_kw in title:
                    score += weight * 2.0  # ì œëª©ì— ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜ 2ë°°
                    matched_kws.append(rank_kw)
                elif rank_kw in content:
                    score += weight * 1.0
                    matched_kws.append(rank_kw)
            
            if score > 0:
                # ê²°ê³¼ ì €ì¥
                matched_article = article.copy()
                matched_article['match_score'] = score
                matched_article['matched_keywords'] = matched_kws
                matched_results.append(matched_article)
        
        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        matched_results.sort(key=lambda x: x['match_score'], reverse=True)
        return matched_results

def main():
    print("ğŸš€ [5ë¶„ ë¸Œë¦¬í•‘] í†µí•© ìƒì„±ê¸° ì‹œì‘\n")
    
    # 1. íŠ¸ë Œë“œ ìˆ˜ì§‘
    collector = TrendCollector()
    
    trends_map = {} # {í‚¤ì›Œë“œ: ì ìˆ˜}
    
    # Google (ê°€ì¤‘ì¹˜ 1.5 - ìµœì‹ ì„± ë†’ìŒ)
    g_trends = collector.get_google_trends()
    for kw in g_trends:
        trends_map[kw] = trends_map.get(kw, 0) + 1.5
        
    # Naver (ê°€ì¤‘ì¹˜ 1.2 - ì¼ë°˜ì  ê´€ì‹¬)
    n_trends = collector.get_naver_datalab_trends()
    for kw in n_trends:
        trends_map[kw] = trends_map.get(kw, 0) + 1.2

    # BlackKiwi (ê°€ì¤‘ì¹˜ 1.8 - ë§ˆì¼€íŒ…/ì‹¤ê²€ ì •í™•ë„ ë†’ìŒ)
    bk_trends = collector.get_blackkiwi_trends()
    for kw in bk_trends['rising']:
        trends_map[kw] = trends_map.get(kw, 0) + 1.8
    for kw in bk_trends['new']:
        trends_map[kw] = trends_map.get(kw, 0) + 1.5

    print(f"\nâœ… ìˆ˜ì§‘ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì´ {len(trends_map)}ê°œ")
    # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸ ì¶œë ¥
    top_5_kws = sorted(trends_map.items(), key=lambda x: x[1], reverse=True)[:5]
    print(f"   (Top 5: {', '.join([k for k,v in top_5_kws])})")

    # 2. ë¡œì»¬ ë‰´ìŠ¤ ë¡œë“œ ë° ë§¤ì¹­
    # ê²½ë¡œ: Crawling â€“ Naver News/news_data.json
    news_path = Path(__file__).resolve().parent.parent / "crawling_naver_news" / "news_data.json"
    
    generator = BriefingGenerator(str(news_path))
    if not generator.news_data:
        print("\nâŒ ì €ì¥ëœ ë‰´ìŠ¤ê°€ ì—†ì–´ ë§¤ì¹­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("   'crawling_naver_news/news_crawler.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    print(f"\nğŸ” ë¡œì»¬ ë‰´ìŠ¤ {len(generator.news_data)}ê±´ ì¤‘ì—ì„œ ë§¤ì¹­ ì¤‘...")
    briefing_articles = generator.match_trends(trends_map)
    
    print(f"âœ… ë§¤ì¹­ëœ ê¸°ì‚¬: {len(briefing_articles)}ê±´")
    
    # 3. ê²°ê³¼ ì¶œë ¥ (Top 5)
    print("\n" + "="*50)
    print("ğŸ“° ì˜¤ëŠ˜ ì•„ì¹¨ [5ë¶„ ë¸Œë¦¬í•‘] ì¶”ì²œ ë‰´ìŠ¤")
    print("="*50)
    
    if not briefing_articles:
        print("âš ï¸ íŠ¸ë Œë“œ í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ” ë‰´ìŠ¤ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.")
        print("   (ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ë„ˆë¬´ ì˜ˆì „ ê²ƒì´ê±°ë‚˜, íŠ¸ë Œë“œì™€ ë¬´ê´€í•œ ì„¹ì…˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    else:
        for i, article in enumerate(briefing_articles[:5], 1):
            print(f"{i}. [{', '.join(article['matched_keywords'])}] {article['title']}")
            print(f"   - {article['press']} | ë°˜ì‘ {article['reaction_count']} | ëŒ“ê¸€ {article['comment_count']}")
            print(f"   - ë§í¬: {article['link']}")
            print(f"   - ìš”ì•½: {article['content'][:50]}...")
            print("-" * 50)

if __name__ == "__main__":
    main()
