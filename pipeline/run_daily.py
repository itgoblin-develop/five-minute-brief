#!/usr/bin/env python3
"""
Five Minute Brief - Daily Pipeline Runner

Unified orchestrator that runs the full pipeline:
  1. Crawling + Ranking + Trend matching (run_batch.py)
  2. AI Reconstruction + Thumbnail + DB loading (reconstruct.py)

Usage:
  python run_daily.py                          # Full run (today KST)
  python run_daily.py --skip-crawl             # Skip crawling, use existing data
  python run_daily.py --dry-run                # No DB loading
  python run_daily.py --date 2026-02-09        # Specific date
  python run_daily.py --skip-crawl --dry-run   # Reconstruction only, no DB
"""

import argparse
import subprocess
import sys
import os
from datetime import datetime, timedelta, timezone
from pathlib import Path

KST = timezone(timedelta(hours=9))
PIPELINE_DIR = Path(__file__).resolve().parent


def log(icon: str, msg: str):
    print(f"{icon} [{datetime.now(KST).strftime('%H:%M:%S')}] {msg}")


def run_step(cmd: list, cwd: Path, timeout: int, label: str) -> bool:
    """Run a subprocess step. Returns True on success."""
    log("â–¶ï¸", f"{label} ì‹œì‘...")
    log("  ", f"cmd: {' '.join(cmd)}")
    try:
        result = subprocess.run(
            cmd,
            cwd=str(cwd),
            timeout=timeout,
            capture_output=False,
        )
        if result.returncode == 0:
            log("âœ…", f"{label} ì™„ë£Œ")
            return True
        else:
            log("âŒ", f"{label} ì‹¤íŒ¨ (exit code: {result.returncode})")
            return False
    except subprocess.TimeoutExpired:
        log("â°", f"{label} íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)")
        return False
    except FileNotFoundError as e:
        log("âŒ", f"{label} ì‹¤í–‰ ë¶ˆê°€: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description="Five Minute Brief - Daily Pipeline Runner")
    parser.add_argument("--skip-crawl", action="store_true", help="Skip crawling, use existing data")
    parser.add_argument("--dry-run", action="store_true", help="Skip DB loading (reconstruct --dry-run)")
    parser.add_argument("--date", type=str, default=None, help="Target date YYYY-MM-DD (default: today KST)")
    args = parser.parse_args()

    # Determine target date in KST
    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y-%m-%d").replace(tzinfo=KST)
        except ValueError:
            print("âŒ --date í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
    else:
        target_date = datetime.now(KST)

    date_str = target_date.strftime("%Y-%m-%d")
    date_compact = target_date.strftime("%Y%m%d")

    # Time window for crawling: yesterday 18:00 ~ today 18:00
    end_dt = target_date.replace(hour=18, minute=0, second=0, microsecond=0)
    start_dt = end_dt - timedelta(days=1)

    print("=" * 60)
    print(f"ğŸ—ï¸  Five Minute Brief - Daily Pipeline")
    print(f"   ë‚ ì§œ: {date_str} (KST)")
    print(f"   ìˆ˜ì§‘ ë²”ìœ„: {start_dt.strftime('%Y-%m-%d %H:%M')} ~ {end_dt.strftime('%Y-%m-%d %H:%M')}")
    print(f"   ëª¨ë“œ: {'DRY-RUN' if args.dry_run else 'PRODUCTION'}")
    print(f"   í¬ë¡¤ë§: {'SKIP' if args.skip_crawl else 'ON'}")
    print("=" * 60)

    crawl_ok = True
    ranking_ok = False
    reconstruct_ok = False

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 1: Crawling + Ranking + Trend Matching
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log("ğŸ“Œ", "Step 1: í¬ë¡¤ë§ + ë­í‚¹ + íŠ¸ë Œë“œ ë§¤ì¹­")

    batch_cmd = [
        sys.executable,
        str(PIPELINE_DIR / "ranking_integrated" / "run_batch.py"),
        "--start", start_dt.strftime("%Y-%m-%d %H:%M"),
        "--end", end_dt.strftime("%Y-%m-%d %H:%M"),
    ]
    if args.skip_crawl:
        batch_cmd.append("--skip-crawl")

    crawl_ok = run_step(
        cmd=batch_cmd,
        cwd=PIPELINE_DIR / "ranking_integrated",
        timeout=600,
        label="í¬ë¡¤ë§ + ë­í‚¹",
    )

    # Check if daily_brief file was generated
    brief_path = PIPELINE_DIR / f"daily_brief_{date_compact}.json"
    # Also check in ranking_integrated directory
    alt_brief_path = PIPELINE_DIR / "ranking_integrated" / f"daily_brief_{date_compact}.json"

    if not brief_path.exists() and alt_brief_path.exists():
        brief_path = alt_brief_path

    if not brief_path.exists():
        if crawl_ok:
            log("âš ï¸", f"daily_brief_{date_compact}.json íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            log("âš ï¸", "í¬ë¡¤ë§ ì‹¤íŒ¨. ê¸°ì¡´ daily_brief íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤...")

        # Try to find any existing daily_brief file
        existing = sorted(PIPELINE_DIR.glob("daily_brief_*.json"), reverse=True)
        if not existing:
            existing = sorted((PIPELINE_DIR / "ranking_integrated").glob("daily_brief_*.json"), reverse=True)

        if existing:
            brief_path = existing[0]
            log("ğŸ“‚", f"ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©: {brief_path.name}")
        else:
            log("âŒ", "ì‚¬ìš© ê°€ëŠ¥í•œ daily_brief íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨.")
            sys.exit(1)
    else:
        log("ğŸ“‚", f"ì…ë ¥ íŒŒì¼: {brief_path.name}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 2: AI Reconstruction + Thumbnail + DB
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print()
    log("ğŸ“Œ", "Step 2: AI ì¬êµ¬ì„± + ì¸ë„¤ì¼ + DB ì ì¬")

    reconstruct_cmd = [
        sys.executable,
        str(PIPELINE_DIR / "reconstruction" / "reconstruct.py"),
        "--input", str(brief_path),
    ]
    if args.dry_run:
        reconstruct_cmd.append("--dry-run")

    # Also save output JSON for artifact upload
    output_json = PIPELINE_DIR / f"reconstructed_{date_compact}.json"
    reconstruct_cmd.extend(["--output", str(output_json)])

    reconstruct_ok = run_step(
        cmd=reconstruct_cmd,
        cwd=PIPELINE_DIR / "reconstruction",
        timeout=900,
        label="AI ì¬êµ¬ì„±",
    )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Summary
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print()
    print("=" * 60)
    if reconstruct_ok and crawl_ok:
        log("ğŸ‰", "íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ëª¨ë“  ë‹¨ê³„ ì„±ê³µ)")
        sys.exit(0)
    elif reconstruct_ok and not crawl_ok:
        log("âš ï¸", "íŒŒì´í”„ë¼ì¸ ë¶€ë¶„ ì™„ë£Œ (í¬ë¡¤ë§ ì‹¤íŒ¨, ì¬êµ¬ì„± ì„±ê³µ)")
        sys.exit(2)
    else:
        log("âŒ", "íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ (ì¬êµ¬ì„± ì‹¤íŒ¨)")
        sys.exit(1)


if __name__ == "__main__":
    main()
