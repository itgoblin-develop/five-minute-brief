"""
ê°œë°œ êµ¬í˜„ ëª©í‘œ
---------------
- **Phase 3 (íŠ¸ë Œë“œ ë¶„ì„ ë° ë­í‚¹)ì˜ ìµœì†Œ ì‹¤í–‰ ë²„ì „**ì„ ë§Œë“¤ì–´ì„œ,
  - ë°ì´í„°ë© + ë‰´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ ëª¨ì€ ê²½ì œ ë‰´ìŠ¤ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ
  - ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ 
  - `TREND_POLICY.md` 3-2ì— ë‚˜ì˜¨ ìŠ¤ì½”ì–´ ê³µì‹ì„ ì°¸ê³ í•´
  - \"ì–´ë–¤ í‚¤ì›Œë“œê°€ ì˜¤ëŠ˜ ê²½ì œ ë¶„ì•¼ì—ì„œ ê°€ì¥ ì„¼ ì´ìŠˆì¸ì§€\"ë¥¼ ì½˜ì†”ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

ë¬´ì—‡ì„ í–ˆëŠ”ì§€
---------------
1. (ë°ëª¨ìš©) ê²½ì œ ëŒ€í‘œ í‚¤ì›Œë“œ 5ê°œ ì¤‘ì—ì„œ ë°ì´í„°ë©ì„ ì´ìš©í•´ ìƒìœ„ 2ê°œ í‚¤ì›Œë“œë¥¼ ê³ ë¥¸ë‹¤.
2. ì´ 2ê°œ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•´, ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ë“¤ì„ ëª¨ì€ë‹¤.
3. ìˆ˜ì§‘ëœ ë‰´ìŠ¤ì˜ ì œëª©/ìš”ì•½ì—ì„œ ê°„ë‹¨í•œ ê·œì¹™ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•œë‹¤.
4. ê° í‚¤ì›Œë“œì— ëŒ€í•´
   - ë¹ˆë„ìˆ˜
   - ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜
   - ì¶œì²˜ ë‹¤ì–‘ì„±(ë‹¤ë¥¸ ì–¸ë¡ ì‚¬ ìˆ˜)
   - ì‹œê°„ ì‹ ì„ ë„(ìµœê·¼ ë‰´ìŠ¤ ë¹„ì¤‘)
   ë¥¼ ê³„ì‚°í•´, `TREND_POLICY.md`ì˜ ìŠ¤ì½”ì–´ ê³µì‹ì„ ì°¸ê³ í•œ ì ìˆ˜ë¥¼ ë§Œë“ ë‹¤.
5. ìµœì¢…ì ìœ¼ë¡œ ìƒìœ„ 10~15ê°œ í‚¤ì›Œë“œì™€, ê° í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” ëŒ€í‘œ ë‰´ìŠ¤ë“¤ì„ ì¶œë ¥í•œë‹¤.

ì–´ë–»ê²Œ í–ˆëŠ”ì§€
---------------
- `step2_datalab_to_news.py`ì—ì„œ ì‚¬ìš©í•œ **ì¸ì¦/ë°ì´í„°ë©/ë‰´ìŠ¤ ê²€ìƒ‰ í•¨ìˆ˜ë“¤ì„ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©**í•œë‹¤.
- í‚¤ì›Œë“œ ì¶”ì¶œ:
  - HTML íƒœê·¸ ì œê±° í›„, ë„ì–´ì“°ê¸° ê¸°ì¤€ ë¶„ë¦¬
  - í•œê¸€ 2ê¸€ì ì´ìƒ & ë¶ˆìš©ì–´(stopwords) ì œì™¸
  - ì˜ë¬¸ ëŒ€ë¬¸ì 3ê¸€ì ì´ìƒ(ETF, GDP ë“±)ì€ í‚¤ì›Œë“œë¡œ ì¸ì •
- ìŠ¤ì½”ì–´ë§:
  - `í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ = (ë¹ˆë„ìˆ˜ Ã— 0.4) + (ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ Ã— 0.3) + (ì¶œì²˜ ë‹¤ì–‘ì„± Ã— 0.2) + (ì‹œê°„ ì‹ ì„ ë„ Ã— 0.1)`
  - ì‹œê°„ ì‹ ì„ ë„ëŠ” \"ìµœê·¼ 6ì‹œê°„ ì´ë‚´/12ì‹œê°„/24ì‹œê°„\"ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë¶€ì—¬ í›„ í‰ê· ê°’ ì‚¬ìš©

í…ŒìŠ¤íŠ¸ ë°©ë²•
---------------
1. í„°ë¯¸ë„ ì—´ê¸°
   - `Command + Space` â†’ `Terminal` ê²€ìƒ‰ â†’ ì‹¤í–‰

2. í”„ë¡œì íŠ¸ í´ë”ë¡œ ì´ë™
   - `cd /Users/nahyojin/Documents/GitHub/five-minute-brief`

3. (ì²˜ìŒ í•œ ë²ˆë§Œ) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
   - `pip3 install -r requirements.txt`

4. Phase 3 ìƒ˜í”Œ ì‹¤í–‰
   - `python3 phase3_trend_scoring.py`

5. ì½˜ì†”ì—ì„œ í™•ì¸í•  ê²ƒ
   - \"[ì„ ì •ëœ ê²½ì œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ TOP 2]\"ê°€ ë¨¼ì € ë‚˜ì˜¤ê³ 
   - ì´ì–´ì„œ \"ğŸ”¥ Phase 3 - ê²½ì œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë­í‚¹\" ì•„ë˜ì— ìƒìœ„ í‚¤ì›Œë“œ ëª©ë¡ì´ ì¶œë ¥ë˜ëŠ”ì§€
   - ê° í‚¤ì›Œë“œë³„ ëŒ€í‘œ ë‰´ìŠ¤ ì œëª©ë“¤ì„ ë³´ê³ , \"ì˜¤ëŠ˜ ê²½ì œ ì´ìŠˆë¥¼ ì˜ ìš”ì•½í•˜ê³  ìˆë‹¤\"ëŠ” ëŠë‚Œì´ ë“œëŠ”ì§€
"""

import re
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from typing import DefaultDict, Dict, List, Optional, Tuple
from difflib import SequenceMatcher

from step2_datalab_to_news import (
    fetch_datalab_economic_trends,
    load_credentials,
    search_news_for_keyword,
)


def extract_keywords_from_text(text: str) -> List[str]:
    """ê°„ë‹¨í•œ ê·œì¹™ìœ¼ë¡œ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•œë‹¤."""
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r"<[^>]+>", "", text)

    # ìì£¼ ë‚˜ì˜¤ëŠ” ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤ì€ ì œì™¸ (í…ŒìŠ¤íŠ¸ìš© ë¶ˆìš©ì–´)
    stopwords = {
        "ì˜¤ëŠ˜",
        "ë‚´ì¼",
        "ì–´ì œ",
        "ì˜",
        "ì´",
        "ê°€",
        "ì„",
        "ë¥¼",
        "ì€",
        "ëŠ”",
        "ì—",
        "ì—ì„œ",
        "ë¡œ",
        "ìœ¼ë¡œ",
        "ì™€",
        "ê³¼",
        "ë„",
        "ë§Œ",
        "ê¹Œì§€",
        "ë¶€í„°",
        "ë³´ë‹¤",
        "ê°™ì´",
        "ê·¸ë¦¬ê³ ",
        "í•˜ì§€ë§Œ",
        "ìµœê·¼",
        "í˜„ì¬",
        "ëŒ€í•œ",
        "ê´€ë ¨",
        "í†µí•´",
        "êµ­ë‚´",
        "ê°œìµœ",
        "ì •ë¶€",
        "ì‹œì¥",
    }

    # ì¡°ì‚¬ ì œê±° íŒ¨í„´ (í•œê¸€ ë‹¨ì–´ ë’¤ì— ë¶™ëŠ” ì¡°ì‚¬ë“¤)
    josa_pattern = re.compile(r"([ê°€-í£]+)([ì´ê°€ì„ë¥¼ì€ëŠ”ì—ì—ì„œë¡œìœ¼ë¡œì™€ê³¼ë„ë§Œê¹Œì§€ë¶€í„°ë³´ë‹¤ê°™ì´]+)$")

    words = text.split()
    keywords: List[str] = []

    for w in words:
        # ì¡°ì‚¬ ì œê±°: "í‚¤ì›Œë“œê°€" -> "í‚¤ì›Œë“œ", "í•œêµ­ì„" -> "í•œêµ­"
        match = josa_pattern.match(w)
        if match:
            w = match.group(1)  # ì¡°ì‚¬ ì œê±°í•œ ìˆœìˆ˜ ë‹¨ì–´ë§Œ ì‚¬ìš©

        # í•œê¸€ 2ê¸€ì ì´ìƒ + ë¶ˆìš©ì–´ ì œì™¸
        if re.match(r"^[ê°€-í£]{2,}$", w) and w not in stopwords:
            keywords.append(w)
        # ì˜ë¬¸ ëŒ€ë¬¸ì 3ê¸€ì ì´ìƒ (ì˜ˆ: ETF, GDP, FOMC)
        elif re.match(r"^[A-Z]{3,}$", w):
            keywords.append(w)

    return keywords


def normalize_keywords(keyword_list: List[str]) -> List[str]:
    """
    ì˜ë¯¸ì ìœ¼ë¡œ ì¤‘ë³µë˜ëŠ” í‚¤ì›Œë“œë“¤ì„ ì •ê·œí™”í•œë‹¤.
    - í¬í•¨ ê´€ê³„: "ê´€ì°°ëŒ€ìƒêµ­"ì´ ìˆìœ¼ë©´ "ê´€ì°°" ì œê±°
    - ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ(ê¸´ ê²ƒ)ë¥¼ ìš°ì„ í•˜ê³ , ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ(ì§§ì€ ê²ƒ)ëŠ” ì œê±°
    """
    if not keyword_list:
        return []

    # í‚¤ì›Œë“œë¥¼ ê¸¸ì´ ìˆœìœ¼ë¡œ ì •ë ¬ (ê¸´ ê²ƒ = ë” êµ¬ì²´ì ì¸ ê²ƒ ìš°ì„ )
    sorted_kw = sorted(set(keyword_list), key=len, reverse=True)

    normalized: List[str] = []

    for kw in sorted_kw:
        # ì´ë¯¸ ì¶”ê°€ëœ í‚¤ì›Œë“œ ì¤‘ì— í˜„ì¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        is_contained = False
        for existing in normalized:
            # existingì´ ë” ê¸´ í‚¤ì›Œë“œì´ê³ , kwê°€ ê·¸ ì•ˆì— í¬í•¨ë˜ë©´ ì œê±°
            if kw in existing and kw != existing:
                is_contained = True
                break

        # í˜„ì¬ í‚¤ì›Œë“œê°€ ì´ë¯¸ ì¶”ê°€ëœ í‚¤ì›Œë“œë“¤ì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
        # (ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œê°€ ë‚˜ì™”ìœ¼ë¯€ë¡œ ê¸°ì¡´ ì¼ë°˜ í‚¤ì›Œë“œ ì œê±°)
        to_remove = []
        for existing in normalized:
            if existing in kw and kw != existing:
                to_remove.append(existing)

        for rm in to_remove:
            normalized.remove(rm)

        if not is_contained:
            normalized.append(kw)

    return normalized


def compute_time_freshness(pub_date: datetime, now: datetime) -> float:
    """ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ì´ ì–¼ë§ˆë‚˜ ìµœê·¼ì¸ì§€ì— ë”°ë¼ 0~1 ì‚¬ì´ì˜ ì‹ ì„ ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤."""
    diff = now - pub_date
    hours = diff.total_seconds() / 3600

    if hours <= 6:
        return 1.0
    if hours <= 12:
        return 0.7
    if hours <= 24:
        return 0.4
    return 0.1


def parse_pubdate(item: Dict) -> Optional[datetime]:
    """ë‰´ìŠ¤ ì•„ì´í…œì—ì„œ pubDateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜í•œë‹¤."""
    try:
        pub_date_str = item.get("pubDate", "")
        base = pub_date_str.split(" +")[0]
        return datetime.strptime(base, "%a, %d %b %Y %H:%M:%S")
    except Exception:
        return None


def compute_title_similarity(title1: str, title2: str) -> float:
    """
    ë‘ ì œëª©ì˜ ìœ ì‚¬ë„ë¥¼ 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.
    SequenceMatcherë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ìì—´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•œë‹¤.
    """
    # HTML íƒœê·¸ ì œê±° ë° ê³µë°± ì •ê·œí™”
    clean1 = re.sub(r'<[^>]+>', '', title1).strip()
    clean2 = re.sub(r'<[^>]+>', '', title2).strip()
    return SequenceMatcher(None, clean1, clean2).ratio()


def remove_duplicate_news(news_list: List[Dict]) -> List[Dict]:
    """
    TREND_POLICY.md 2-3ì— ë”°ë¥¸ ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§:
    1. ë§í¬ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    2. ì œëª© ìœ ì‚¬ë„ 80% ì´ìƒ ì¤‘ë³µ ì œê±°
    3. ì¶œì²˜ ë‹¤ì–‘ì„± í™•ë³´ (ê°™ì€ ì–¸ë¡ ì‚¬ ë‰´ìŠ¤ë§Œ ëª°ë¦¬ì§€ ì•Šë„ë¡)
    """
    if not news_list:
        return []

    # 1. ë§í¬ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    seen_links: set[str] = set()
    deduped_by_link: List[Dict] = []
    for item in news_list:
        link = item.get("link", "") or item.get("originallink", "")
        if link and link not in seen_links:
            seen_links.add(link)
            deduped_by_link.append(item)

    # 2. ì œëª© ìœ ì‚¬ë„ 80% ì´ìƒ ì¤‘ë³µ ì œê±°
    deduped_by_title: List[Dict] = []
    for item in deduped_by_link:
        title = item.get("title", "")
        is_duplicate = False
        for existing in deduped_by_title:
            existing_title = existing.get("title", "")
            similarity = compute_title_similarity(title, existing_title)
            if similarity >= 0.8:  # 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                is_duplicate = True
                break
        if not is_duplicate:
            deduped_by_title.append(item)

    # 3. ì¶œì²˜ ë‹¤ì–‘ì„± í™•ë³´: ê°™ì€ ì–¸ë¡ ì‚¬ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ ì œí•œ
    # originallinkì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
    def extract_domain(link: str) -> str:
        """ë§í¬ì—ì„œ ë„ë©”ì¸ì„ ì¶”ì¶œí•œë‹¤."""
        if not link:
            return ""
        # http:// ë˜ëŠ” https:// ì œê±° í›„ ì²« ë²ˆì§¸ / ì „ê¹Œì§€ê°€ ë„ë©”ì¸
        cleaned = link.replace("http://", "").replace("https://", "")
        domain = cleaned.split("/")[0]
        return domain

    source_count: DefaultDict[str, int] = defaultdict(int)
    final_news: List[Dict] = []
    # ê°™ì€ ì¶œì²˜ë‹¹ ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ í—ˆìš© (ì¶œì²˜ ë‹¤ì–‘ì„± í™•ë³´)
    MAX_NEWS_PER_SOURCE = 3

    for item in deduped_by_title:
        link = item.get("link", "") or item.get("originallink", "")
        domain = extract_domain(link)
        if source_count[domain] < MAX_NEWS_PER_SOURCE:
            source_count[domain] += 1
            final_news.append(item)

    return final_news


def analyze_trends(news_list: List[Dict]) -> List[Dict]:
    """
    ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ê³ ,
    ê° í‚¤ì›Œë“œì˜ ìŠ¤ì½”ì–´ì™€ ëŒ€í‘œ ë‰´ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤.
    """
    # TREND_POLICY.md 2-3: ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§ ì ìš©
    filtered_news = remove_duplicate_news(news_list)

    now = datetime.now()

    freq: Counter = Counter()
    news_mapping: DefaultDict[str, List[Dict]] = defaultdict(list)
    source_mapping: DefaultDict[str, set] = defaultdict(set)
    freshness_mapping: DefaultDict[str, List[float]] = defaultdict(list)

    for item in filtered_news:
        title = item.get("title", "")
        desc = item.get("description", "")
        link = item.get("link", "") or item.get("originallink", "")
        origin_link = item.get("originallink", "") or link

        title_kws = extract_keywords_from_text(title)
        desc_kws = extract_keywords_from_text(desc)

        pub_dt = parse_pubdate(item)
        freshness = compute_time_freshness(pub_dt, now) if pub_dt else 0.5

        # ì œëª© í‚¤ì›Œë“œëŠ” ê°€ì¤‘ì¹˜ 3
        for kw in title_kws:
            freq[kw] += 3
            if item not in news_mapping[kw]:
                news_mapping[kw].append(item)
            source_mapping[kw].add(origin_link)
            freshness_mapping[kw].append(freshness)

        # ìš”ì•½ í‚¤ì›Œë“œëŠ” ê°€ì¤‘ì¹˜ 1
        for kw in desc_kws:
            freq[kw] += 1
            if item not in news_mapping[kw]:
                news_mapping[kw].append(item)
            source_mapping[kw].add(origin_link)
            freshness_mapping[kw].append(freshness)

    # í‚¤ì›Œë“œ ì •ê·œí™”: ì¼ë‹¨ ë¹„í™œì„±í™” (ë””ë²„ê¹…ìš©)
    # ì •ê·œí™” ì—†ì´ ì›ë³¸ í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    all_keywords = list(freq.keys())
    
    # ë””ë²„ê¹…: ì›ë³¸ í‚¤ì›Œë“œ ê°œìˆ˜ ë° ìƒìœ„ í‚¤ì›Œë“œ í™•ì¸
    print(f"   [ë””ë²„ê¹…] ì¶”ì¶œëœ ì›ë³¸ í‚¤ì›Œë“œ ê°œìˆ˜: {len(all_keywords)}")
    if len(all_keywords) > 0:
        # ë¹ˆë„ìˆ˜ ê¸°ì¤€ ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ì¶œë ¥
        top_keywords = sorted(freq.items(), key=lambda x: x[1], reverse=True)[:20]
        print(f"   [ë””ë²„ê¹…] ì›ë³¸ í‚¤ì›Œë“œ ëª©ë¡ (ë¹ˆë„ìˆ˜ ìƒìœ„ 20ê°œ):")
        for kw, cnt in top_keywords:
            print(f"      - {kw}: {cnt}íšŒ")

    # ì •ê·œí™” ë¹„í™œì„±í™”: ëª¨ë“  í‚¤ì›Œë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    keyword_mapping: Dict[str, str] = {kw: kw for kw in all_keywords}

    # ì •ê·œí™” ì—†ì´ ì›ë³¸ í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    merged_freq = freq.copy()
    merged_news_mapping = news_mapping.copy()
    merged_source_mapping = source_mapping.copy()
    merged_freshness_mapping = freshness_mapping.copy()

    results: List[Dict] = []

    # í•„í„°ë§ ì „ ìƒíƒœ í™•ì¸
    total_keywords_before_filter = len([kw for kw in merged_freq.keys() if merged_news_mapping.get(kw)])
    print(f"   [ë””ë²„ê¹…] í•„í„°ë§ ì „ í‚¤ì›Œë“œ ê°œìˆ˜: {total_keywords_before_filter}")
    
    for kw, count in merged_freq.most_common():
        related_news = merged_news_mapping.get(kw, [])
        if not related_news:
            continue

        news_count = len(related_news)
        source_count = len(merged_source_mapping.get(kw, set()))
        freshness_list = merged_freshness_mapping.get(kw, [0.5])
        avg_freshness = sum(freshness_list) / len(freshness_list) if freshness_list else 0.5

        # í•„í„°ë§ ì¡°ê±´ ì™„í™”: ë‰´ìŠ¤ 1ê°œ ì´ìƒ, ë¹ˆë„ìˆ˜ 3 ì´ìƒ
        if news_count < 1 or count < 3:
            continue

        score = (
            count * 0.4
            + news_count * 0.3
            + source_count * 0.2
            + avg_freshness * 0.1
        )

        # ìµœì‹  ë‰´ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•´ì„œ ìƒìœ„ 3ê°œë§Œ ëŒ€í‘œ ë‰´ìŠ¤ë¡œ ì‚¬ìš©
        sorted_news = sorted(
            related_news,
            key=lambda n: n.get("pubDate", ""),
            reverse=True,
        )[:3]

        results.append(
            {
                "keyword": kw,
                "score": score,
                "frequency": count,
                "news_count": news_count,
                "source_count": source_count,
                "avg_freshness": avg_freshness,
                "top_news": sorted_news,
            }
        )

    # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    results.sort(key=lambda x: x["score"], reverse=True)
    
    # í•„í„°ë§ í›„ ìƒíƒœ í™•ì¸
    print(f"   [ë””ë²„ê¹…] í•„í„°ë§ í›„ ìµœì¢… í‚¤ì›Œë“œ ê°œìˆ˜: {len(results)}")
    if len(results) > 0:
        print(f"   [ë””ë²„ê¹…] ìµœì¢… í‚¤ì›Œë“œ ëª©ë¡ (ìƒìœ„ 10ê°œ):")
        for i, r in enumerate(results[:10], 1):
            print(f"      {i}. {r['keyword']} (ë¹ˆë„={r['frequency']}, ë‰´ìŠ¤={r['news_count']}, ì ìˆ˜={r['score']:.2f})")
    
    return results


def run_phase3_sample() -> None:
    """ê²½ì œ ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ìœ¼ë¡œ Phase 3 íŠ¸ë Œë“œ ë¶„ì„ ìƒ˜í”Œì„ ì‹¤í–‰í•œë‹¤."""
    client_id, client_secret = load_credentials()

    print("=" * 60)
    print("ğŸ§ª Phase 3 ìƒ˜í”Œ: ê²½ì œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë­í‚¹")
    print("=" * 60)

    # 1) ë°ì´í„°ë©ìœ¼ë¡œ ê²½ì œ ëŒ€í‘œ í‚¤ì›Œë“œ 5ê°œ ì¤‘ ìƒìœ„ 2ê°œ ì„ íƒ
    print("\n[1] ë°ì´í„°ë© - ê²½ì œ ëŒ€í‘œ í‚¤ì›Œë“œ 5ê°œ íŠ¸ë Œë“œ ì¡°íšŒ")
    ratios = fetch_datalab_economic_trends(client_id, client_secret)
    if not ratios:
        print("âš ï¸ ë°ì´í„°ë©ì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("   í‚¤ì›Œë“œë³„ ratio (ì–´ì œ ê¸°ì¤€):")
    for kw, r in ratios:
        print(f"   - {kw}: {r}")

    top2 = sorted(ratios, key=lambda x: x[1], reverse=True)[:2]

    print("\n[ì„ ì •ëœ ê²½ì œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ TOP 2]")
    for kw, r in top2:
        print(f"   - {kw} (ratio={r})")

    # 2) ì„ ì •ëœ í‚¤ì›Œë“œë“¤ë¡œ ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘
    print("\n[2] ë‰´ìŠ¤ ìˆ˜ì§‘ - TOP 2 í‚¤ì›Œë“œ ê¸°ì¤€ ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ ëª¨ìœ¼ê¸° (í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 20ê°œ)")
    all_news: List[Dict] = []
    seen_links: set[str] = set()

    for kw, _ in top2:
        print(f"\n   ğŸ” í‚¤ì›Œë“œ: {kw}")
        news_list = search_news_for_keyword(
            client_id,
            client_secret,
            keyword=kw,
            hours=24,
            max_news=20,
        )
        print(f"      ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê°œìˆ˜: {len(news_list)}")

        for item in news_list:
            link = item.get("link", "") or item.get("originallink", "")
            if not link or link in seen_links:
                continue
            seen_links.add(link)
            all_news.append(item)

    print("\n[ìš”ì•½] ì „ì²´ ìˆ˜ì§‘ ë‰´ìŠ¤ ê°œìˆ˜:", len(all_news))
    if not all_news:
        print("âš ï¸ ë¶„ì„í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3) Phase 3 ìŠ¤ì½”ì–´ë§ ìˆ˜í–‰ (ì¤‘ë³µ ì œê±° í¬í•¨)
    print("\n[3] Phase 3 ìŠ¤ì½”ì–´ë§ - í‚¤ì›Œë“œ ë¶„ì„ ë° ë­í‚¹ ê³„ì‚° ì¤‘...")
    print("   (TREND_POLICY.md 2-3 ì ìš©: ë§í¬ ì¤‘ë³µ ì œê±°, ì œëª© ìœ ì‚¬ë„ 80% ì´ìƒ ì œê±°, ì¶œì²˜ ë‹¤ì–‘ì„± í™•ë³´)")
    trending = analyze_trends(all_news)

    print("\n" + "=" * 60)
    print("ğŸ”¥ Phase 3 - ê²½ì œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë­í‚¹ (ìƒìœ„ 10ê°œ)")
    print("=" * 60)

    if not trending:
        print("âš ï¸ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    for idx, item in enumerate(trending[:10], start=1):
        print(f"\n{idx}. í‚¤ì›Œë“œ: {item['keyword']}")
        print(f"   ì ìˆ˜: {item['score']:.2f}")
        print(f"   ë¹ˆë„ìˆ˜: {item['frequency']}, ê´€ë ¨ ë‰´ìŠ¤: {item['news_count']}ê°œ, ì¶œì²˜: {item['source_count']}ê°œ")
        print(f"   í‰ê·  ì‹œê°„ ì‹ ì„ ë„: {item['avg_freshness']:.2f}")
        print("   ëŒ€í‘œ ë‰´ìŠ¤:")
        for i, n in enumerate(item["top_news"], start=1):
            title = n.get("title", "").replace("<b>", "").replace("</b>", "")
            pub_date = n.get("pubDate", "")
            link = n.get("link", "")
            print(f"      {i}. {title}")
            print(f"         ë°œí–‰: {pub_date}")
            print(f"         ë§í¬: {link}")


if __name__ == "__main__":
    run_phase3_sample()

